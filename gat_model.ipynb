{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhcGj3AO6OnA",
        "outputId": "aa0621e2-bbce-4d24-e48d-f04cfbf1f100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAHkUnC_7Qmu",
        "outputId": "f6636ccc-cd76-43d1-ce84-a4c2c240eded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/661.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/661.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m593.9/661.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=89f5b450bb527b1c0956d5f54d835a8f05230dcfd42cbd5250db4b4a6f6b9361\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4PPTs92iR0r",
        "outputId": "b3656c44-539b-4671-9588-b304ead446ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=361a60e468334eaeb1ae6d14423845f379537076255f8c9cebb6f1f71d34193d\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.33.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQP_NxPa-OAI",
        "outputId": "ab740967-8a70-400f-fd59-ac23d7c31580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Labeled Dataset/\")\n",
        "from data_loader import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "0I1WJU3LBvju",
        "outputId": "f76ace2d-a713-480c-8960-c53bf259d574"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-43241c9c5c6a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGATConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#improved GAT+SBERT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/Labeled Dataset/long_sentiments.csv'\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# class weights\n",
        "class_counts = df['label'].value_counts().to_dict()\n",
        "total_samples = len(df)\n",
        "class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
        "weights = torch.tensor([class_weights[i] for i in range(3)], dtype=torch.float).to(device)\n",
        "\n",
        "# Initialize Sentence-BERT model\n",
        "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "parent_embeddings = sbert_model.encode(df['body_parent'].tolist())\n",
        "child_embeddings = sbert_model.encode(df['body_child'].tolist())\n",
        "\n",
        "# Create node features and edges\n",
        "\n",
        "# Sentence-BERT embeddings\n",
        "parent_embeddings = sbert_model.encode(df['body_parent'].tolist())\n",
        "child_embeddings = sbert_model.encode(df['body_child'].tolist())\n",
        "\n",
        "# Convert to PyTorch tensors and concatenate sentiment scores\n",
        "node_features = torch.cat((\n",
        "    torch.tensor(parent_embeddings, dtype=torch.float),\n",
        "    torch.tensor(child_embeddings, dtype=torch.float),\n",
        "    torch.tensor(df['sentiment_parent'].values.reshape(-1, 1), dtype=torch.float),\n",
        "    torch.tensor(df['sentiment_child'].values.reshape(-1, 1), dtype=torch.float)\n",
        "), dim=1)\n",
        "\n",
        "# Create edges\n",
        "all_msg_ids = pd.concat([df['msg_id_parent'], df['msg_id_child']]).unique()\n",
        "msg_to_int = {msg: i for i, msg in enumerate(all_msg_ids)}\n",
        "edges = torch.tensor([(msg_to_int[parent], msg_to_int[child]) for parent, child in zip(df['msg_id_parent'], df['msg_id_child'])], dtype=torch.long).t().contiguous()\n",
        "\n",
        "# Labels and masks\n",
        "labels = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "n_samples = len(df)\n",
        "train_mask = torch.zeros(n_samples, dtype=torch.bool)\n",
        "val_mask = torch.zeros(n_samples, dtype=torch.bool)\n",
        "test_mask = torch.zeros(n_samples, dtype=torch.bool)\n",
        "\n",
        "train_indices = torch.randperm(n_samples)[:int(0.7 * n_samples)]\n",
        "val_indices = torch.randperm(n_samples)[int(0.7 * n_samples):int(0.85 * n_samples)]\n",
        "test_indices = torch.randperm(n_samples)[int(0.85 * n_samples):]\n",
        "train_mask[train_indices] = True\n",
        "val_mask[val_indices] = True\n",
        "test_mask[test_indices] = True\n",
        "\n",
        "\n",
        "data = Data(x=node_features, edge_index=edges, y=labels, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "# Hybrid Model definition\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.gat1 = GATConv(in_channels, 64, heads=8, dropout=0.6)\n",
        "        self.gat2 = GATConv(64 * 8, 64, heads=1, concat=False, dropout=0.6)\n",
        "        self.fc = nn.Linear(64 + in_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x\n",
        "        edge_index = data.edge_index\n",
        "        x_gat = F.dropout(x, p=0.6, training=self.training)\n",
        "        x_gat = F.elu(self.gat1(x_gat, edge_index))\n",
        "        x_gat = F.dropout(x_gat, p=0.6, training=self.training)\n",
        "        x_gat = self.gat2(x_gat, edge_index)\n",
        "        x_concat = torch.cat((x_gat, x), dim=1)\n",
        "        x_out = self.fc(x_concat)\n",
        "        return F.log_softmax(x_out, dim=1)\n",
        "\n",
        "\n",
        "model = HybridModel(node_features.shape[1], 3).to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "\n",
        "# early stopping logic\n",
        "PATIENCE = 20\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "def train(data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], weight=weights)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(data, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(data).max(dim=1)[1]\n",
        "    correct = preds[mask].eq(data.y[mask]).sum().item()\n",
        "    return correct / mask.sum().item(), preds\n",
        "\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "losses = []\n",
        "\n",
        "for epoch in range(200):\n",
        "    loss = train(data)\n",
        "    train_acc = evaluate(data, data.train_mask)[0]\n",
        "    val_acc = evaluate(data, data.val_mask)[0]\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    losses.append(loss)\n",
        "\n",
        "    print(f\"Epoch: {epoch + 1}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping logic\n",
        "    if loss < best_val_loss:\n",
        "        best_val_loss = loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= PATIENCE:\n",
        "            print(\"Early Stopping triggered.\")\n",
        "            break\n",
        "\n",
        "\n",
        "acc, predictions = evaluate(data, data.test_mask)\n",
        "predictions = predictions[data.test_mask].cpu().numpy()\n",
        "ground_truth = data.y[data.test_mask].cpu().numpy()\n",
        "\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(ground_truth, predictions))\n",
        "\n",
        "# Confusion matrix\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, cmap=plt.cm.Blues):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=\"Confusion Matrix\",\n",
        "           ylabel=\"True label\",\n",
        "           xlabel=\"Predicted label\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_confusion_matrix(ground_truth, predictions, classes=[0, 1, 2])\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "print(\"Model and results saved.\")\n",
        "\n",
        "# Optionally, plot training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(losses, label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb0woZrz_dLR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Path of dataset\n",
        "path = '/content/drive/MyDrive/Labeled Dataset/long_sentiments.csv'\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Calculate class weights for the loss function\n",
        "class_counts = df['label'].value_counts().to_dict()\n",
        "total_samples = len(df)\n",
        "class_weights = {\n",
        "    cls: total_samples / count for cls, count in class_counts.items()\n",
        "}\n",
        "weights = torch.tensor([class_weights[i] for i in range(3)], dtype=torch.float).to(device)\n",
        "\n",
        "# Initialize Sentence-BERT model\n",
        "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "parent_embeddings = sbert_model.encode(df['body_parent'].tolist())\n",
        "child_embeddings = sbert_model.encode(df['body_child'].tolist())\n",
        "\n",
        ".\n",
        "\n",
        "# Revised Hybrid Model\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.gat1 = GATConv(in_channels, 64, heads=8, dropout=0.6)\n",
        "        self.gat2 = GATConv(64 * 8, 64, heads=1, concat=False, dropout=0.6)\n",
        "        self.fc = nn.Linear(64 + in_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x\n",
        "        edge_index = data.edge_index\n",
        "\n",
        "        x_gat = F.dropout(x, p=0.6, training=self.training)\n",
        "        x_gat = F.elu(self.gat1(x_gat, edge_index))\n",
        "        x_gat = F.dropout(x_gat, p=0.6, training=self.training)\n",
        "        x_gat = self.gat2(x_gat, edge_index)\n",
        "\n",
        "        x_concat = torch.cat((x_gat, x), dim=1)\n",
        "        x_out = self.fc(x_concat)\n",
        "\n",
        "        return F.log_softmax(x_out, dim=1)\n",
        "\n",
        "\n",
        "model = HybridModel(node_features.shape[1], 3).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDe75VeoYIka",
        "outputId": "651973c3-9a99-41db-ba27-e7dad5946094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 1.3953735828399658\n",
            "Epoch: 2, Loss: 1.4017583131790161\n",
            "Epoch: 3, Loss: 1.3502089977264404\n",
            "Epoch: 4, Loss: 1.304420828819275\n",
            "Epoch: 5, Loss: 1.2966735363006592\n",
            "Epoch: 6, Loss: 1.2937790155410767\n",
            "Epoch: 7, Loss: 1.263827919960022\n",
            "Epoch: 8, Loss: 1.2281451225280762\n",
            "Epoch: 9, Loss: 1.231244683265686\n",
            "Epoch: 10, Loss: 1.22014582157135\n",
            "Epoch: 11, Loss: 1.2045432329177856\n",
            "Epoch: 12, Loss: 1.1898020505905151\n",
            "Epoch: 13, Loss: 1.1845554113388062\n",
            "Epoch: 14, Loss: 1.1802911758422852\n",
            "Epoch: 15, Loss: 1.161356806755066\n",
            "Epoch: 16, Loss: 1.1575177907943726\n",
            "Epoch: 17, Loss: 1.1552152633666992\n",
            "Epoch: 18, Loss: 1.1366277933120728\n",
            "Epoch: 19, Loss: 1.1308834552764893\n",
            "Epoch: 20, Loss: 1.1349384784698486\n",
            "Epoch: 21, Loss: 1.134308934211731\n",
            "Epoch: 22, Loss: 1.1238973140716553\n",
            "Epoch: 23, Loss: 1.1200578212738037\n",
            "Epoch: 24, Loss: 1.1180676221847534\n",
            "Epoch: 25, Loss: 1.1120542287826538\n",
            "Epoch: 26, Loss: 1.1115210056304932\n",
            "Epoch: 27, Loss: 1.1109188795089722\n",
            "Epoch: 28, Loss: 1.1114052534103394\n",
            "Epoch: 29, Loss: 1.1115617752075195\n",
            "Epoch: 30, Loss: 1.1087515354156494\n",
            "Epoch: 31, Loss: 1.109473466873169\n",
            "Epoch: 32, Loss: 1.1079509258270264\n",
            "Epoch: 33, Loss: 1.1055326461791992\n",
            "Epoch: 34, Loss: 1.10848867893219\n",
            "Epoch: 35, Loss: 1.1044179201126099\n",
            "Epoch: 36, Loss: 1.1050400733947754\n",
            "Epoch: 37, Loss: 1.1036514043807983\n",
            "Epoch: 38, Loss: 1.1034903526306152\n",
            "Epoch: 39, Loss: 1.1015987396240234\n",
            "Epoch: 40, Loss: 1.102712869644165\n",
            "Epoch: 41, Loss: 1.1048343181610107\n",
            "Epoch: 42, Loss: 1.104203701019287\n",
            "Epoch: 43, Loss: 1.1021252870559692\n",
            "Epoch: 44, Loss: 1.1018081903457642\n",
            "Epoch: 45, Loss: 1.1027188301086426\n",
            "Epoch: 46, Loss: 1.1001708507537842\n",
            "Epoch: 47, Loss: 1.103775978088379\n",
            "Epoch: 48, Loss: 1.100631594657898\n",
            "Epoch: 49, Loss: 1.1031420230865479\n",
            "Epoch: 50, Loss: 1.100746989250183\n",
            "Epoch: 51, Loss: 1.1000903844833374\n",
            "Epoch: 52, Loss: 1.0997332334518433\n",
            "Epoch: 53, Loss: 1.1000702381134033\n",
            "Epoch: 54, Loss: 1.0992978811264038\n",
            "Epoch: 55, Loss: 1.1001540422439575\n",
            "Epoch: 56, Loss: 1.0994796752929688\n",
            "Epoch: 57, Loss: 1.1006577014923096\n",
            "Epoch: 58, Loss: 1.0997148752212524\n",
            "Epoch: 59, Loss: 1.0991706848144531\n",
            "Epoch: 60, Loss: 1.099530816078186\n",
            "Epoch: 61, Loss: 1.100257158279419\n",
            "Epoch: 62, Loss: 1.09902024269104\n",
            "Epoch: 63, Loss: 1.1003550291061401\n",
            "Epoch: 64, Loss: 1.1002804040908813\n",
            "Epoch: 65, Loss: 1.100868821144104\n",
            "Epoch: 66, Loss: 1.0999116897583008\n",
            "Epoch: 67, Loss: 1.1000572443008423\n",
            "Epoch: 68, Loss: 1.1003599166870117\n",
            "Epoch: 69, Loss: 1.0998399257659912\n",
            "Epoch: 70, Loss: 1.0992635488510132\n",
            "Epoch: 71, Loss: 1.0996676683425903\n",
            "Epoch: 72, Loss: 1.098998785018921\n",
            "Epoch: 73, Loss: 1.0999339818954468\n",
            "Epoch: 74, Loss: 1.0986180305480957\n",
            "Epoch: 75, Loss: 1.100035548210144\n",
            "Epoch: 76, Loss: 1.0994616746902466\n",
            "Epoch: 77, Loss: 1.0997148752212524\n",
            "Epoch: 78, Loss: 1.099915623664856\n",
            "Epoch: 79, Loss: 1.098875641822815\n",
            "Epoch: 80, Loss: 1.0987329483032227\n",
            "Epoch: 81, Loss: 1.0986526012420654\n",
            "Epoch: 82, Loss: 1.0990735292434692\n",
            "Epoch: 83, Loss: 1.0993247032165527\n",
            "Epoch: 84, Loss: 1.099371314048767\n",
            "Epoch: 85, Loss: 1.0995709896087646\n",
            "Epoch: 86, Loss: 1.0996124744415283\n",
            "Epoch: 87, Loss: 1.09968900680542\n",
            "Epoch: 88, Loss: 1.0996971130371094\n",
            "Epoch: 89, Loss: 1.0988355875015259\n",
            "Epoch: 90, Loss: 1.098561406135559\n",
            "Epoch: 91, Loss: 1.098242998123169\n",
            "Epoch: 92, Loss: 1.099324107170105\n",
            "Epoch: 93, Loss: 1.09978449344635\n",
            "Epoch: 94, Loss: 1.0993683338165283\n",
            "Epoch: 95, Loss: 1.0986350774765015\n",
            "Epoch: 96, Loss: 1.100035309791565\n",
            "Epoch: 97, Loss: 1.098615288734436\n",
            "Epoch: 98, Loss: 1.0984416007995605\n",
            "Epoch: 99, Loss: 1.0991848707199097\n",
            "Epoch: 100, Loss: 1.0994935035705566\n",
            "Epoch: 101, Loss: 1.0999634265899658\n",
            "Epoch: 102, Loss: 1.09954833984375\n",
            "Epoch: 103, Loss: 1.0988463163375854\n",
            "Epoch: 104, Loss: 1.0989460945129395\n",
            "Epoch: 105, Loss: 1.0982917547225952\n",
            "Epoch: 106, Loss: 1.099422812461853\n",
            "Epoch: 107, Loss: 1.0984089374542236\n",
            "Epoch: 108, Loss: 1.0983905792236328\n",
            "Epoch: 109, Loss: 1.0988281965255737\n",
            "Epoch: 110, Loss: 1.0986149311065674\n",
            "Epoch: 111, Loss: 1.0990018844604492\n",
            "Epoch: 112, Loss: 1.0991246700286865\n",
            "Epoch: 113, Loss: 1.0991179943084717\n",
            "Epoch: 114, Loss: 1.0994211435317993\n",
            "Epoch: 115, Loss: 1.0983517169952393\n",
            "Epoch: 116, Loss: 1.099197268486023\n",
            "Epoch: 117, Loss: 1.0998353958129883\n",
            "Epoch: 118, Loss: 1.0984764099121094\n",
            "Epoch: 119, Loss: 1.0987874269485474\n",
            "Epoch: 120, Loss: 1.0987012386322021\n",
            "Epoch: 121, Loss: 1.0986218452453613\n",
            "Epoch: 122, Loss: 1.0991733074188232\n",
            "Epoch: 123, Loss: 1.0986908674240112\n",
            "Epoch: 124, Loss: 1.09888756275177\n",
            "Epoch: 125, Loss: 1.0994116067886353\n",
            "Epoch: 126, Loss: 1.098388671875\n",
            "Epoch: 127, Loss: 1.0995086431503296\n",
            "Epoch: 128, Loss: 1.0989134311676025\n",
            "Epoch: 129, Loss: 1.098723292350769\n",
            "Epoch: 130, Loss: 1.0988880395889282\n",
            "Epoch: 131, Loss: 1.0988441705703735\n",
            "Epoch: 132, Loss: 1.098943829536438\n",
            "Epoch: 133, Loss: 1.0995376110076904\n",
            "Epoch: 134, Loss: 1.09859037399292\n",
            "Epoch: 135, Loss: 1.0987640619277954\n",
            "Epoch: 136, Loss: 1.0989991426467896\n",
            "Epoch: 137, Loss: 1.0987416505813599\n",
            "Epoch: 138, Loss: 1.098127007484436\n",
            "Epoch: 139, Loss: 1.098218560218811\n",
            "Epoch: 140, Loss: 1.0992134809494019\n",
            "Epoch: 141, Loss: 1.0985959768295288\n",
            "Epoch: 142, Loss: 1.0988315343856812\n",
            "Epoch: 143, Loss: 1.0987927913665771\n",
            "Epoch: 144, Loss: 1.0986226797103882\n",
            "Epoch: 145, Loss: 1.0991071462631226\n",
            "Epoch: 146, Loss: 1.0987637042999268\n",
            "Epoch: 147, Loss: 1.0987002849578857\n",
            "Epoch: 148, Loss: 1.0990127325057983\n",
            "Epoch: 149, Loss: 1.0985376834869385\n",
            "Epoch: 150, Loss: 1.0988554954528809\n",
            "Epoch: 151, Loss: 1.09823739528656\n",
            "Epoch: 152, Loss: 1.0996718406677246\n",
            "Epoch: 153, Loss: 1.09868586063385\n",
            "Epoch: 154, Loss: 1.0992869138717651\n",
            "Epoch: 155, Loss: 1.0991109609603882\n",
            "Epoch: 156, Loss: 1.0990076065063477\n",
            "Epoch: 157, Loss: 1.0988532304763794\n",
            "Epoch: 158, Loss: 1.098926305770874\n",
            "Epoch: 159, Loss: 1.0988185405731201\n",
            "Epoch: 160, Loss: 1.0990159511566162\n",
            "Epoch: 161, Loss: 1.099114179611206\n",
            "Epoch: 162, Loss: 1.0981650352478027\n",
            "Epoch: 163, Loss: 1.098985195159912\n",
            "Epoch: 164, Loss: 1.0988699197769165\n",
            "Epoch: 165, Loss: 1.0988616943359375\n",
            "Epoch: 166, Loss: 1.098992109298706\n",
            "Epoch: 167, Loss: 1.0985966920852661\n",
            "Epoch: 168, Loss: 1.0989035367965698\n",
            "Epoch: 169, Loss: 1.0989750623703003\n",
            "Epoch: 170, Loss: 1.0990265607833862\n",
            "Epoch: 171, Loss: 1.0991075038909912\n",
            "Epoch: 172, Loss: 1.0993971824645996\n",
            "Epoch: 173, Loss: 1.0988465547561646\n",
            "Epoch: 174, Loss: 1.0989967584609985\n",
            "Epoch: 175, Loss: 1.0988178253173828\n",
            "Epoch: 176, Loss: 1.0987741947174072\n",
            "Epoch: 177, Loss: 1.098929762840271\n",
            "Epoch: 178, Loss: 1.0986047983169556\n",
            "Epoch: 179, Loss: 1.0988306999206543\n",
            "Epoch: 180, Loss: 1.0985026359558105\n",
            "Epoch: 181, Loss: 1.0983883142471313\n",
            "Epoch: 182, Loss: 1.0987036228179932\n",
            "Epoch: 183, Loss: 1.099036693572998\n",
            "Epoch: 184, Loss: 1.0988554954528809\n",
            "Epoch: 185, Loss: 1.0991427898406982\n",
            "Epoch: 186, Loss: 1.0989265441894531\n",
            "Epoch: 187, Loss: 1.0986908674240112\n",
            "Epoch: 188, Loss: 1.098842740058899\n",
            "Epoch: 189, Loss: 1.0990179777145386\n",
            "Epoch: 190, Loss: 1.098884105682373\n",
            "Epoch: 191, Loss: 1.0989549160003662\n",
            "Epoch: 192, Loss: 1.0989636182785034\n",
            "Epoch: 193, Loss: 1.0988529920578003\n",
            "Epoch: 194, Loss: 1.0986292362213135\n",
            "Epoch: 195, Loss: 1.0986162424087524\n",
            "Epoch: 196, Loss: 1.0992449522018433\n",
            "Epoch: 197, Loss: 1.0991230010986328\n",
            "Epoch: 198, Loss: 1.0985862016677856\n",
            "Epoch: 199, Loss: 1.0987153053283691\n",
            "Epoch: 200, Loss: 1.0988233089447021\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.04      0.08      4243\n",
            "           1       0.17      0.40      0.24      1418\n",
            "           2       0.36      0.58      0.45      3060\n",
            "\n",
            "    accuracy                           0.29      8721\n",
            "   macro avg       0.33      0.34      0.25      8721\n",
            "weighted avg       0.38      0.29      0.23      8721\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#GAT alone\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "\n",
        "# Set random seeds\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/Labeled Dataset/long_sentiments.csv'\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Class weighting\n",
        "class_counts = df['label'].value_counts().to_dict()\n",
        "total_samples = len(df)\n",
        "class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
        "weights = torch.tensor([class_weights[i] for i in range(3)], dtype=torch.float).to(device)\n",
        "\n",
        "# Feature Normalization\n",
        "scaler = StandardScaler()\n",
        "node_features = np.stack((df['sentiment_parent'].values, df['sentiment_child'].values), axis=1)\n",
        "node_features = scaler.fit_transform(node_features)\n",
        "\n",
        "\n",
        "msg_to_int = {msg: i for i, msg in enumerate(pd.concat([df['msg_id_parent'], df['msg_id_child']]).unique())}\n",
        "edges = torch.tensor([(msg_to_int[parent], msg_to_int[child]) for parent, child in zip(df['msg_id_parent'], df['msg_id_child'])], dtype=torch.long).t().contiguous()\n",
        "\n",
        "node_features = torch.tensor(node_features, dtype=torch.float)\n",
        "labels = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "\n",
        "data = Data(x=node_features, edge_index=edges, y=labels).to(device)\n",
        "\n",
        "# Model\n",
        "class GATClassifier(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GATClassifier, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, 64, heads=8, dropout=0.6)\n",
        "        self.conv2 = GATConv(64 * 8, 32, heads=4, dropout=0.6)\n",
        "        self.conv3 = GATConv(32 * 4, out_channels, heads=1, concat=False, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = F.dropout(data.x, p=0.6, training=self.training)\n",
        "        x = F.elu(self.conv1(x, data.edge_index))\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = F.elu(self.conv2(x, data.edge_index))\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv3(x, data.edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = GATClassifier(node_features.shape[1], 3).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.7)\n",
        "\n",
        "# Training\n",
        "def train(data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out, data.y, weight=weights)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Evaluation\n",
        "def evaluate(data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(data).max(dim=1)[1]\n",
        "    return preds\n",
        "\n",
        "# Main Loop\n",
        "for epoch in range(200):\n",
        "    loss = train(data)\n",
        "    preds = evaluate(data)\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch: {epoch + 1}, Loss: {loss}\")\n",
        "\n",
        "# Evaluation\n",
        "preds = evaluate(data).cpu().numpy()\n",
        "labels = data.y.cpu().numpy()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(labels, preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKZfiB9t5yuV",
        "outputId": "e00666b7-16c4-498e-c3fa-57bbde3c9856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OTm0-7T_5Dy-",
        "outputId": "421483a1-bcb2-4b93-c8c0-5b358827df70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 2.0225, Train Acc: 0.3526, Val Acc: 0.3593\n",
            "Epoch: 2, Loss: 1.8711, Train Acc: 0.3458, Val Acc: 0.3502\n",
            "Epoch: 3, Loss: 1.8656, Train Acc: 0.3468, Val Acc: 0.3502\n",
            "Epoch: 4, Loss: 1.7644, Train Acc: 0.3503, Val Acc: 0.3502\n",
            "Epoch: 5, Loss: 1.7083, Train Acc: 0.3779, Val Acc: 0.3677\n",
            "Epoch: 6, Loss: 1.6624, Train Acc: 0.4622, Val Acc: 0.4557\n",
            "Epoch: 7, Loss: 1.6443, Train Acc: 0.4700, Val Acc: 0.4450\n",
            "Epoch: 8, Loss: 1.5557, Train Acc: 0.4477, Val Acc: 0.4205\n",
            "Epoch: 9, Loss: 1.5360, Train Acc: 0.4253, Val Acc: 0.4006\n",
            "Epoch: 10, Loss: 1.5436, Train Acc: 0.3784, Val Acc: 0.3869\n",
            "Epoch: 11, Loss: 1.4519, Train Acc: 0.3571, Val Acc: 0.3647\n",
            "Epoch: 12, Loss: 1.4419, Train Acc: 0.3550, Val Acc: 0.3624\n",
            "Epoch: 13, Loss: 1.4151, Train Acc: 0.3568, Val Acc: 0.3631\n",
            "Epoch: 14, Loss: 1.3844, Train Acc: 0.3609, Val Acc: 0.3693\n",
            "Epoch: 15, Loss: 1.3534, Train Acc: 0.3701, Val Acc: 0.3777\n",
            "Epoch: 16, Loss: 1.3637, Train Acc: 0.3891, Val Acc: 0.3899\n",
            "Epoch: 17, Loss: 1.3721, Train Acc: 0.4092, Val Acc: 0.4044\n",
            "Epoch: 18, Loss: 1.3108, Train Acc: 0.4263, Val Acc: 0.4174\n",
            "Epoch: 19, Loss: 1.2838, Train Acc: 0.4649, Val Acc: 0.4480\n",
            "Epoch: 20, Loss: 1.3008, Train Acc: 0.4798, Val Acc: 0.4557\n",
            "Epoch: 21, Loss: 1.3011, Train Acc: 0.4802, Val Acc: 0.4580\n",
            "Epoch: 22, Loss: 1.2930, Train Acc: 0.4690, Val Acc: 0.4534\n",
            "Epoch: 23, Loss: 1.2535, Train Acc: 0.4574, Val Acc: 0.4427\n",
            "Epoch: 24, Loss: 1.2930, Train Acc: 0.4355, Val Acc: 0.4312\n",
            "Epoch: 25, Loss: 1.2598, Train Acc: 0.4274, Val Acc: 0.4258\n",
            "Epoch: 26, Loss: 1.2747, Train Acc: 0.4253, Val Acc: 0.4190\n",
            "Epoch: 27, Loss: 1.2616, Train Acc: 0.4471, Val Acc: 0.4373\n",
            "Epoch: 28, Loss: 1.2469, Train Acc: 0.4784, Val Acc: 0.4648\n",
            "Epoch: 29, Loss: 1.2296, Train Acc: 0.5057, Val Acc: 0.4839\n",
            "Epoch: 30, Loss: 1.2535, Train Acc: 0.5318, Val Acc: 0.5023\n",
            "Epoch: 31, Loss: 1.2239, Train Acc: 0.5398, Val Acc: 0.4985\n",
            "Epoch: 32, Loss: 1.2011, Train Acc: 0.5490, Val Acc: 0.5099\n",
            "Epoch: 33, Loss: 1.2276, Train Acc: 0.5614, Val Acc: 0.5176\n",
            "Epoch: 34, Loss: 1.2113, Train Acc: 0.5624, Val Acc: 0.5168\n",
            "Epoch: 35, Loss: 1.2045, Train Acc: 0.5590, Val Acc: 0.5214\n",
            "Epoch: 36, Loss: 1.2092, Train Acc: 0.5531, Val Acc: 0.5245\n",
            "Epoch: 37, Loss: 1.2001, Train Acc: 0.5464, Val Acc: 0.5115\n",
            "Epoch: 38, Loss: 1.1987, Train Acc: 0.5400, Val Acc: 0.5038\n",
            "Epoch: 39, Loss: 1.1844, Train Acc: 0.5382, Val Acc: 0.5023\n",
            "Epoch: 40, Loss: 1.1846, Train Acc: 0.5388, Val Acc: 0.5061\n",
            "Epoch: 41, Loss: 1.1944, Train Acc: 0.5490, Val Acc: 0.5138\n",
            "Epoch: 42, Loss: 1.1815, Train Acc: 0.5678, Val Acc: 0.5268\n",
            "Epoch: 43, Loss: 1.1739, Train Acc: 0.5760, Val Acc: 0.5321\n",
            "Epoch: 44, Loss: 1.1675, Train Acc: 0.5714, Val Acc: 0.5252\n",
            "Epoch: 45, Loss: 1.1881, Train Acc: 0.5686, Val Acc: 0.5291\n",
            "Epoch: 46, Loss: 1.1794, Train Acc: 0.5624, Val Acc: 0.5222\n",
            "Epoch: 47, Loss: 1.1617, Train Acc: 0.5624, Val Acc: 0.5252\n",
            "Epoch: 48, Loss: 1.1593, Train Acc: 0.5704, Val Acc: 0.5291\n",
            "Epoch: 49, Loss: 1.1468, Train Acc: 0.5862, Val Acc: 0.5459\n",
            "Epoch: 50, Loss: 1.1641, Train Acc: 0.5935, Val Acc: 0.5520\n",
            "Epoch: 51, Loss: 1.1614, Train Acc: 0.5960, Val Acc: 0.5520\n",
            "Epoch: 52, Loss: 1.1395, Train Acc: 0.5832, Val Acc: 0.5550\n",
            "Epoch: 53, Loss: 1.1378, Train Acc: 0.5768, Val Acc: 0.5489\n",
            "Epoch: 54, Loss: 1.1409, Train Acc: 0.5857, Val Acc: 0.5550\n",
            "Epoch: 55, Loss: 1.1416, Train Acc: 0.5911, Val Acc: 0.5543\n",
            "Epoch: 56, Loss: 1.1353, Train Acc: 0.6001, Val Acc: 0.5550\n",
            "Epoch: 57, Loss: 1.1309, Train Acc: 0.5973, Val Acc: 0.5505\n",
            "Epoch: 58, Loss: 1.1467, Train Acc: 0.5998, Val Acc: 0.5528\n",
            "Epoch: 59, Loss: 1.1182, Train Acc: 0.6075, Val Acc: 0.5558\n",
            "Epoch: 60, Loss: 1.1345, Train Acc: 0.6152, Val Acc: 0.5627\n",
            "Epoch: 61, Loss: 1.1190, Train Acc: 0.6135, Val Acc: 0.5665\n",
            "Epoch: 62, Loss: 1.1318, Train Acc: 0.6144, Val Acc: 0.5703\n",
            "Epoch: 63, Loss: 1.1111, Train Acc: 0.6153, Val Acc: 0.5696\n",
            "Epoch: 64, Loss: 1.1142, Train Acc: 0.6173, Val Acc: 0.5726\n",
            "Epoch: 65, Loss: 1.1096, Train Acc: 0.6004, Val Acc: 0.5627\n",
            "Epoch: 66, Loss: 1.0964, Train Acc: 0.5970, Val Acc: 0.5535\n",
            "Epoch: 67, Loss: 1.1037, Train Acc: 0.5990, Val Acc: 0.5550\n",
            "Epoch: 68, Loss: 1.1180, Train Acc: 0.5988, Val Acc: 0.5612\n",
            "Epoch: 69, Loss: 1.1074, Train Acc: 0.6062, Val Acc: 0.5688\n",
            "Epoch: 70, Loss: 1.1012, Train Acc: 0.6204, Val Acc: 0.5719\n",
            "Epoch: 71, Loss: 1.1063, Train Acc: 0.6289, Val Acc: 0.5795\n",
            "Epoch: 72, Loss: 1.0882, Train Acc: 0.6381, Val Acc: 0.5841\n",
            "Epoch: 73, Loss: 1.1006, Train Acc: 0.6334, Val Acc: 0.5803\n",
            "Epoch: 74, Loss: 1.0956, Train Acc: 0.6245, Val Acc: 0.5772\n",
            "Epoch: 75, Loss: 1.0803, Train Acc: 0.6157, Val Acc: 0.5680\n",
            "Epoch: 76, Loss: 1.1000, Train Acc: 0.6052, Val Acc: 0.5642\n",
            "Epoch: 77, Loss: 1.0963, Train Acc: 0.6216, Val Acc: 0.5757\n",
            "Epoch: 78, Loss: 1.1001, Train Acc: 0.6298, Val Acc: 0.5742\n",
            "Epoch: 79, Loss: 1.0834, Train Acc: 0.6448, Val Acc: 0.5887\n",
            "Epoch: 80, Loss: 1.0887, Train Acc: 0.6476, Val Acc: 0.5956\n",
            "Epoch: 81, Loss: 1.0839, Train Acc: 0.6466, Val Acc: 0.5956\n",
            "Epoch: 82, Loss: 1.0770, Train Acc: 0.6419, Val Acc: 0.5803\n",
            "Epoch: 83, Loss: 1.0859, Train Acc: 0.6329, Val Acc: 0.5780\n",
            "Epoch: 84, Loss: 1.0752, Train Acc: 0.6180, Val Acc: 0.5711\n",
            "Epoch: 85, Loss: 1.0770, Train Acc: 0.6109, Val Acc: 0.5665\n",
            "Epoch: 86, Loss: 1.0847, Train Acc: 0.6148, Val Acc: 0.5665\n",
            "Epoch: 87, Loss: 1.0709, Train Acc: 0.6343, Val Acc: 0.5795\n",
            "Epoch: 88, Loss: 1.0716, Train Acc: 0.6425, Val Acc: 0.5803\n",
            "Epoch: 89, Loss: 1.0735, Train Acc: 0.6443, Val Acc: 0.5887\n",
            "Epoch: 90, Loss: 1.0714, Train Acc: 0.6524, Val Acc: 0.6032\n",
            "Epoch: 91, Loss: 1.0880, Train Acc: 0.6527, Val Acc: 0.6040\n",
            "Epoch: 92, Loss: 1.0621, Train Acc: 0.6530, Val Acc: 0.5971\n",
            "Epoch: 93, Loss: 1.0724, Train Acc: 0.6494, Val Acc: 0.5956\n",
            "Epoch: 94, Loss: 1.0669, Train Acc: 0.6394, Val Acc: 0.5795\n",
            "Epoch: 95, Loss: 1.0618, Train Acc: 0.6325, Val Acc: 0.5749\n",
            "Epoch: 96, Loss: 1.0760, Train Acc: 0.6276, Val Acc: 0.5726\n",
            "Epoch: 97, Loss: 1.0582, Train Acc: 0.6348, Val Acc: 0.5749\n",
            "Epoch: 98, Loss: 1.0545, Train Acc: 0.6452, Val Acc: 0.5818\n",
            "Epoch: 99, Loss: 1.0520, Train Acc: 0.6597, Val Acc: 0.6055\n",
            "Epoch: 100, Loss: 1.0496, Train Acc: 0.6640, Val Acc: 0.6116\n",
            "Epoch: 101, Loss: 1.0578, Train Acc: 0.6646, Val Acc: 0.6078\n",
            "Epoch: 102, Loss: 1.0552, Train Acc: 0.6581, Val Acc: 0.6009\n",
            "Epoch: 103, Loss: 1.0483, Train Acc: 0.6465, Val Acc: 0.5917\n",
            "Epoch: 104, Loss: 1.0645, Train Acc: 0.6281, Val Acc: 0.5765\n",
            "Epoch: 105, Loss: 1.0623, Train Acc: 0.6320, Val Acc: 0.5749\n",
            "Epoch: 106, Loss: 1.0567, Train Acc: 0.6394, Val Acc: 0.5803\n",
            "Epoch: 107, Loss: 1.0443, Train Acc: 0.6543, Val Acc: 0.5948\n",
            "Epoch: 108, Loss: 1.0449, Train Acc: 0.6653, Val Acc: 0.6024\n",
            "Epoch: 109, Loss: 1.0411, Train Acc: 0.6594, Val Acc: 0.5994\n",
            "Epoch: 110, Loss: 1.0534, Train Acc: 0.6520, Val Acc: 0.5940\n",
            "Epoch: 111, Loss: 1.0383, Train Acc: 0.6447, Val Acc: 0.5910\n",
            "Epoch: 112, Loss: 1.0434, Train Acc: 0.6407, Val Acc: 0.5940\n",
            "Epoch: 113, Loss: 1.0496, Train Acc: 0.6437, Val Acc: 0.5940\n",
            "Epoch: 114, Loss: 1.0467, Train Acc: 0.6556, Val Acc: 0.5979\n",
            "Epoch: 115, Loss: 1.0352, Train Acc: 0.6601, Val Acc: 0.6017\n",
            "Epoch: 116, Loss: 1.0358, Train Acc: 0.6715, Val Acc: 0.6109\n",
            "Epoch: 117, Loss: 1.0438, Train Acc: 0.6704, Val Acc: 0.6093\n",
            "Epoch: 118, Loss: 1.0419, Train Acc: 0.6668, Val Acc: 0.6063\n",
            "Epoch: 119, Loss: 1.0435, Train Acc: 0.6624, Val Acc: 0.6070\n",
            "Epoch: 120, Loss: 1.0334, Train Acc: 0.6566, Val Acc: 0.6055\n",
            "Early Stopping triggered.\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.63      0.69       630\n",
            "           1       0.41      0.71      0.52       206\n",
            "           2       0.64      0.58      0.61       473\n",
            "\n",
            "    accuracy                           0.62      1309\n",
            "   macro avg       0.60      0.64      0.60      1309\n",
            "weighted avg       0.66      0.62      0.63      1309\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4wAAAMpCAYAAABPG73/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABgeklEQVR4nO3dd5RV9bk//vcZlKEOiArIFVEUFBRLMCpRUSNWYonkayOKuZagYKLYYmzYQr6axHYNfm+uLV69pqlJ1IgFS6LYRVGMV7FAIoOJBhCUNpzfH8r8MnKIzAAe5vB6nbXXmrPb5zlnrc3Mw/Pszy4Ui8ViAAAA4DOqyh0AAAAAqycJIwAAACVJGAEAAChJwggAAEBJEkYAAABKkjACAABQkoQRAACAkiSMAAAAlCRhBAAAoCQJIwCN9vrrr2fvvfdOhw4dUigUctddd63U87/99tspFAq56aabVup5m7Pdd989u+++e7nDAGANI2EEaKamTJmSb3/72+nZs2datWqVmpqa7Lzzzrnqqqvy8ccfr9Kxhw0blkmTJuXSSy/NLbfcku23336VjvdFOuaYY1IoFFJTU1Pye3z99ddTKBRSKBTyox/9qNHnf/fddzN69OhMnDhxJUQLAKvWWuUOAIDGu+eee/J//s//SXV1dY4++uhstdVWWbBgQf70pz/ljDPOyCuvvJL//M//XCVjf/zxx5kwYULOOeecjBw5cpWM0aNHj3z88cdZe+21V8n5P89aa62Vjz76KL///e9z6KGHNth26623plWrVpk3b16Tzv3uu+/mwgsvzMYbb5xtt912uY+7//77mzQeAKwICSNAM/PWW2/l8MMPT48ePTJ+/PhssMEG9dtGjBiRN954I/fcc88qG/9vf/tbkqRjx46rbIxCoZBWrVqtsvN/nurq6uy88875n//5n6USxttuuy2DBw/Ob37zmy8klo8++iht2rRJy5Ytv5DxAOCfaUkFaGYuu+yyzJkzJ9dff32DZHGJzTbbLN/97nfr3y9atCgXX3xxNt1001RXV2fjjTfO97///cyfP7/BcRtvvHG+9rWv5U9/+lN22GGHtGrVKj179szPf/7z+n1Gjx6dHj16JEnOOOOMFAqFbLzxxkk+aeVc8vM/Gz16dAqFQoN1DzzwQHbZZZd07Ngx7dq1y+abb57vf//79duXdQ/j+PHjs+uuu6Zt27bp2LFjDjrooLz66qslx3vjjTdyzDHHpGPHjunQoUO+9a1v5aOPPlr2F/sZRx55ZP7whz9k5syZ9eueeeaZvP766znyyCOX2v+DDz7I6aefnn79+qVdu3apqanJfvvtlxdffLF+n0ceeSRf/vKXkyTf+ta36ltbl3zO3XffPVtttVWee+65DBw4MG3atKn/Xj57D+OwYcPSqlWrpT7/Pvvsk3XWWSfvvvvucn9WAFgWCSNAM/P73/8+PXv2zFe+8pXl2v+4447L+eefny996Uu54oorsttuu2XMmDE5/PDDl9r3jTfeyDe+8Y3stdde+fGPf5x11lknxxxzTF555ZUkySGHHJIrrrgiSXLEEUfklltuyZVXXtmo+F955ZV87Wtfy/z583PRRRflxz/+cQ488MA8/vjj//K4Bx98MPvss0/ee++9jB49OqNGjcoTTzyRnXfeOW+//fZS+x966KH58MMPM2bMmBx66KG56aabcuGFFy53nIccckgKhULuuOOO+nW33XZbtthii3zpS19aav8333wzd911V772ta/lJz/5Sc4444xMmjQpu+22W33y1qdPn1x00UVJkhNOOCG33HJLbrnllgwcOLD+PO+//37222+/bLvttrnyyiuzxx57lIzvqquuyvrrr59hw4alrq4uSfL//t//y/33359rrrkm3bp1W+7PCgDLVASg2Zg1a1YxSfGggw5arv0nTpxYTFI87rjjGqw//fTTi0mK48ePr1/Xo0ePYpLiY489Vr/uvffeK1ZXVxdPO+20+nVvvfVWMUnx8ssvb3DOYcOGFXv06LFUDBdccEHxn3/dXHHFFcUkxb/97W/LjHvJGDfeeGP9um233bbYuXPn4vvvv1+/7sUXXyxWVVUVjz766KXG+/d///cG5/z6179eXHfddZc55j9/jrZt2xaLxWLxG9/4RnHPPfcsFovFYl1dXbFr167FCy+8sOR3MG/evGJdXd1Sn6O6urp40UUX1a975plnlvpsS+y2227FJMXrrruu5Lbddtutwbpx48YVkxQvueSS4ptvvlls165d8eCDD/7czwgAy0uFEaAZmT17dpKkffv2y7X/vffemyQZNWpUg/WnnXZakix1r2Pfvn2z66671r9ff/31s/nmm+fNN99scsyfteTex9/+9rdZvHjxch0zffr0TJw4Mcccc0w6depUv37rrbfOXnvtVf85/9nw4cMbvN91113z/vvv13+Hy+PII4/MI488ktra2owfPz61tbUl21GTT+57rKr65NdqXV1d3n///fp22+eff365x6yurs63vvWt5dp37733zre//e1cdNFFOeSQQ9KqVav8v//3/5Z7LAD4PBJGgGakpqYmSfLhhx8u1/7vvPNOqqqqstlmmzVY37Vr13Ts2DHvvPNOg/UbbbTRUudYZ5118o9//KOJES/tsMMOy84775zjjjsuXbp0yeGHH55f/vKX/zJ5XBLn5ptvvtS2Pn365O9//3vmzp3bYP1nP8s666yTJI36LPvvv3/at2+fX/ziF7n11lvz5S9/eanvconFixfniiuuSK9evVJdXZ311lsv66+/fl566aXMmjVrucf8t3/7t0ZNcPOjH/0onTp1ysSJE3P11Venc+fOy30sAHweCSNAM1JTU5Nu3brl5ZdfbtRxn510ZllatGhRcn2xWGzyGEvur1uidevWeeyxx/Lggw/mqKOOyksvvZTDDjsse+2111L7rogV+SxLVFdX55BDDsnNN9+cO++8c5nVxST5wQ9+kFGjRmXgwIH57//+74wbNy4PPPBAttxyy+WupCaffD+N8cILL+S9995LkkyaNKlRxwLA55EwAjQzX/va1zJlypRMmDDhc/ft0aNHFi9enNdff73B+hkzZmTmzJn1M56uDOuss06DGUWX+GwVM0mqqqqy55575ic/+UkmT56cSy+9NOPHj8/DDz9c8txL4nzttdeW2vbnP/856623Xtq2bbtiH2AZjjzyyLzwwgv58MMPS04UtMSvf/3r7LHHHrn++utz+OGHZ++9986gQYOW+k6WN3lfHnPnzs23vvWt9O3bNyeccEIuu+yyPPPMMyvt/AAgYQRoZs4888y0bds2xx13XGbMmLHU9ilTpuSqq65K8klLZZKlZjL9yU9+kiQZPHjwSotr0003zaxZs/LSSy/Vr5s+fXruvPPOBvt98MEHSx275AH2n33UxxIbbLBBtt1229x8880NErCXX345999/f/3nXBX22GOPXHzxxfmP//iPdO3adZn7tWjRYqnq5a9+9av89a9/bbBuSWJbKrlurLPOOitTp07NzTffnJ/85CfZeOONM2zYsGV+jwDQWGuVOwAAGmfTTTfNbbfdlsMOOyx9+vTJ0Ucfna222ioLFizIE088kV/96lc55phjkiTbbLNNhg0blv/8z//MzJkzs9tuu+Xpp5/OzTffnIMPPniZj2xoisMPPzxnnXVWvv71r+c73/lOPvroo4wdOza9e/duMOnLRRddlMceeyyDBw9Ojx498t577+WnP/1pNtxww+yyyy7LPP/ll1+e/fbbLwMGDMixxx6bjz/+ONdcc006dOiQ0aNHr7TP8VlVVVU599xzP3e/r33ta7nooovyrW99K1/5ylcyadKk3HrrrenZs2eD/TbddNN07Ngx1113Xdq3b5+2bdtmxx13zCabbNKouMaPH5+f/vSnueCCC+of83HjjTdm9913z3nnnZfLLrusUecDgFJUGAGaoQMPPDAvvfRSvvGNb+S3v/1tRowYke9973t5++238+Mf/zhXX311/b7/9V//lQsvvDDPPPNMTjnllIwfPz5nn312br/99pUa07rrrps777wzbdq0yZlnnpmbb745Y8aMyQEHHLBU7BtttFFuuOGGjBgxItdee20GDhyY8ePHp0OHDss8/6BBg3Lfffdl3XXXzfnnn58f/ehH2WmnnfL44483OtlaFb7//e/ntNNOy7hx4/Ld7343zz//fO6555507969wX5rr712br755rRo0SLDhw/PEUcckUcffbRRY3344Yf593//92y33XY555xz6tfvuuuu+e53v5sf//jHefLJJ1fK5wJgzVYoNubufwAAANYYKowAAACUJGEEAACgJAkjAAAAJUkYAQAAKEnCCAAAQEkSRgAAAEpaq9wBVLrFixfn3XffTfv27VMoFModDgAANFqxWMyHH36Ybt26paqqedWc5s2blwULFpQ7jLRs2TKtWrUqdxiNJmFcxd59992lHtoMAADN0bRp07LhhhuWO4zlNm/evLRuv26y6KNyh5KuXbvmrbfeanZJo4RxFWvfvn2SpGXfYSm0aFnmaIB/du/Pzy13CEAJdcViuUMAPmPunA/z9YH96v+2bS4WLFiQLPoo1X2HJeX8W7xuQWon35wFCxZIGGloSRtqoUVLCSOsZtq2ryl3CEAJdYsljLC6ara3WK3Vqqx/ixcLzauN958138gBAABYpSSMAAAAlKQlFQAAqGyFJOVsp22mnbyJCiMAAADLIGEEAACgJC2pAABAZStUfbKUc/xmqvlGDgAAwColYQQAAKAkLakAAEBlKxTKPEtq850mVYURAACAklQYAQCAymbSmyZrvpEDAACwSkkYAQAAKElLKgAAUNlMetNkKowAAACUJGEEAACgJC2pAABAhSvzLKnNuE7XfCMHAABglVJhBAAAKptJb5pMhREAAICSJIwAAACUpCUVAACobIUyT3pT1gl3VkzzjRwAAIBVSsIIAABASVpSAQCAymaW1CZTYQQAAKAkCSMAAAAlaUkFAAAqm1lSm6z5Rg4AAMAqpcIIAABUNpPeNJkKIwAAACVJGAEAAChJSyoAAFDZTHrTZM03cgAAAFYpCSMAAAAlaUkFAAAqW6FQ5pZUs6QCAABQYVQYAQCAylZV+GQp5/jNlAojAAAAJUkYAQAAKElLKgAAUNk8h7HJmm/kAAAArFISRgAAAErSkgoAAFS2QqG8z0L0HEYAAAAqjQojAABQ2Ux602TNN3IAAABWKQkjAAAAJWlJBQAAKptJb5pMhREAAICSJIwAAACUpCUVAACobGZJbbLmGzkAAACrlIQRAACAkrSkAgAAlc0sqU2mwggAAEBJKowAAEBlM+lNkzXfyAEAAFilJIwAAACUpCUVAACobCa9aTIVRgAAAEqSMAIAAFCSllQAAKDClXmW1GZcp2u+kQMAALBKqTACAACVzaQ3TabCCAAAQEkSRgAAAErSkgoAAFS2QqG8k95oSQUAAKDSSBgBAAAoSUsqAABQ2Qplfg5jWZ8BuWKab+QAAACsUhJGAAAAStKSCgAAVLZCobwzlZolFQAAgEqjwggAAFQ2k940WfONHAAAgFVKwggAAEBJWlIBAIDKZtKbJlNhBAAAoCQJIwAAACVpSQUAACqbWVKbrPlGDgAAUIHGjh2brbfeOjU1NampqcmAAQPyhz/8oX777rvvnkKh0GAZPnx4g3NMnTo1gwcPTps2bdK5c+ecccYZWbRoUaNjUWEEAAAqWzOb9GbDDTfMD3/4w/Tq1SvFYjE333xzDjrooLzwwgvZcsstkyTHH398Lrroovpj2rRpU/9zXV1dBg8enK5du+aJJ57I9OnTc/TRR2fttdfOD37wg0bFImEEAABYjRxwwAEN3l966aUZO3ZsnnzyyfqEsU2bNunatWvJ4++///5Mnjw5Dz74YLp06ZJtt902F198cc4666yMHj06LVu2XO5YtKQCAAB8AWbPnt1gmT9//uceU1dXl9tvvz1z587NgAED6tffeuutWW+99bLVVlvl7LPPzkcffVS/bcKECenXr1+6dOlSv26fffbJ7Nmz88orrzQqZhVGAACgoi25z6+MASRJunfv3mD1BRdckNGjR5c8ZNKkSRkwYEDmzZuXdu3a5c4770zfvn2TJEceeWR69OiRbt265aWXXspZZ52V1157LXfccUeSpLa2tkGymKT+fW1tbaNClzACAAB8AaZNm5aampr699XV1cvcd/PNN8/EiRMza9as/PrXv86wYcPy6KOPpm/fvjnhhBPq9+vXr1822GCD7LnnnpkyZUo23XTTlRqzllQAAIAvwJJZT5cs/yphbNmyZTbbbLP0798/Y8aMyTbbbJOrrrqq5L477rhjkuSNN95IknTt2jUzZsxosM+S98u673FZJIwAAEBF++wjKMqxrKjFixcv857HiRMnJkk22GCDJMmAAQMyadKkvPfee/X7PPDAA6mpqalva11eWlIBAABWI2effXb222+/bLTRRvnwww9z22235ZFHHsm4ceMyZcqU3Hbbbdl///2z7rrr5qWXXsqpp56agQMHZuutt06S7L333unbt2+OOuqoXHbZZamtrc25556bESNG/MuqZikSRgAAgNXIe++9l6OPPjrTp09Phw4dsvXWW2fcuHHZa6+9Mm3atDz44IO58sorM3fu3HTv3j1DhgzJueeeW398ixYtcvfdd+fEE0/MgAED0rZt2wwbNqzBcxuXl4QRAACobIVPl3KO3wjXX3/9Mrd17949jz766Oeeo0ePHrn33nsbN3AJ7mEEAACgJBVGAACgoq0uz2FsjlQYAQAAKEnCCAAAQElaUgEAgIqmJbXpVBgBAAAoScIIAABASVpSAQCAiqYltelUGAEAAChJhREAAKhoKoxNp8IIAABASRJGAAAAStKSCgAAVLbCp0s5x2+mVBgBAAAoqVkkjIVCIXfddVe5wwAAAFijlD1hrK2tzcknn5yePXumuro63bt3zwEHHJCHHnqo3KElSYrFYs4///xssMEGad26dQYNGpTXX3+93GEBAADLacksqeVcmquyJoxvv/12+vfvn/Hjx+fyyy/PpEmTct9992WPPfbIiBEjyhlavcsuuyxXX311rrvuujz11FNp27Zt9tlnn8ybN6/coQEAAKxSZU0YTzrppBQKhTz99NMZMmRIevfunS233DKjRo3Kk08+uczjzjrrrPTu3Ttt2rRJz549c95552XhwoX121988cXssccead++fWpqatK/f/88++yzSZJ33nknBxxwQNZZZ520bds2W265Ze69996S4xSLxVx55ZU599xzc9BBB2XrrbfOz3/+87z77rtaZAEAoJkoFMpdZSz3N9B0ZZsl9YMPPsh9992XSy+9NG3btl1qe8eOHZd5bPv27XPTTTelW7dumTRpUo4//vi0b98+Z555ZpJk6NCh2W677TJ27Ni0aNEiEydOzNprr50kGTFiRBYsWJDHHnssbdu2zeTJk9OuXbuS47z11lupra3NoEGD6td16NAhO+64YyZMmJDDDz98qWPmz5+f+fPn17+fPXv2cn0fAAAAq5uyJYxvvPFGisVitthii0Yfe+6559b/vPHGG+f000/P7bffXp8wTp06NWeccUb9uXv16lW//9SpUzNkyJD069cvSdKzZ89ljlNbW5sk6dKlS4P1Xbp0qd/2WWPGjMmFF17Y6M8EAACwuilbS2qxWGzysb/4xS+y8847p2vXrmnXrl3OPffcTJ06tX77qFGjctxxx2XQoEH54Q9/mClTptRv+853vpNLLrkkO++8cy644IK89NJLK/Q5Puvss8/OrFmz6pdp06at1PMDAACNU0iZJ71pxg9iLFvC2KtXrxQKhfz5z39u1HETJkzI0KFDs//+++fuu+/OCy+8kHPOOScLFiyo32f06NF55ZVXMnjw4IwfPz59+/bNnXfemSQ57rjj8uabb+aoo47KpEmTsv322+eaa64pOVbXrl2TJDNmzGiwfsaMGfXbPqu6ujo1NTUNFgAAgOaobAljp06dss8+++Taa6/N3Llzl9o+c+bMksc98cQT6dGjR84555xsv/326dWrV955552l9uvdu3dOPfXU3H///TnkkENy44031m/r3r17hg8fnjvuuCOnnXZafvazn5Uca5NNNknXrl0bPOJj9uzZeeqppzJgwIBGfmIAAIDmpayzpF577bWpq6vLDjvskN/85jd5/fXX8+qrr+bqq69eZkLWq1evTJ06NbfffnumTJmSq6++ur56mCQff/xxRo4cmUceeSTvvPNOHn/88TzzzDPp06dPkuSUU07JuHHj8tZbb+X555/Pww8/XL/tswqFQk455ZRccskl+d3vfpdJkybl6KOPTrdu3XLwwQev9O8DAABY+cr9DMbm/BzGsk16k3wy4czzzz+fSy+9NKeddlqmT5+e9ddfP/3798/YsWNLHnPggQfm1FNPzciRIzN//vwMHjw45513XkaPHp0kadGiRd5///0cffTRmTFjRtZbb70ccsgh9RPR1NXVZcSIEfnLX/6Smpqa7LvvvrniiiuWGeOZZ56ZuXPn5oQTTsjMmTOzyy675L777kurVq1W+vcBAACwOikUV2T2GT7X7Nmz06FDh1T3Oz6FFi3LHQ7wTx759SXlDgEooW6xP01gdTN3zuzs/aWNM2vWrGY1R8eSv8XXOey/UmjZpmxxFBd8lH/84rhm9/0lZa4wAgAArHKFT5dyjt9MlfUeRgAAAFZfKowAAEBlK/PEM8VmPOmNCiMAAAAlSRgBAAAoSUsqAABQ0cr9LMTm/BxGFUYAAABKkjACAABQkpZUAACgomlJbToVRgAAAEpSYQQAACpb4dOlnOM3UyqMAAAAlCRhBAAAoCQtqQAAQEUz6U3TqTACAABQkoQRAACAkrSkAgAAFU1LatOpMAIAAFCShBEAAICStKQCAAAVTUtq06kwAgAAUJIKIwAAUNFUGJtOhREAAICSJIwAAACUpCUVAACobIVPl3KO30ypMAIAAFCShBEAAICStKQCAAAVzSypTafCCAAAQEkqjAAAQEVTYWw6FUYAAABKkjACAABQkpZUAACgomlJbToVRgAAAEqSMAIAAFCSllQAAKCyFT5dyjl+M6XCCAAAQEkqjAAAQEUz6U3TqTACAABQkoQRAACAkrSkAgAAFU1LatOpMAIAAFCShBEAAICStKQCAAAVrZAyt6Q24wcxqjACAABQkoQRAACAkrSkAgAAFc0sqU2nwggAAEBJKowAAEBlK3y6lHP8ZkqFEQAAgJIkjAAAAJSkJRUAAKhoJr1pOhVGAAAASpIwAgAAUJKWVAAAoKJpSW06FUYAAIDVyNixY7P11lunpqYmNTU1GTBgQP7whz/Ub583b15GjBiRddddN+3atcuQIUMyY8aMBueYOnVqBg8enDZt2qRz584544wzsmjRokbHImEEAAAqWqFQ/qUxNtxww/zwhz/Mc889l2effTZf/epXc9BBB+WVV15Jkpx66qn5/e9/n1/96ld59NFH8+677+aQQw6pP76uri6DBw/OggUL8sQTT+Tmm2/OTTfdlPPPP7/R352WVAAAgNXIAQcc0OD9pZdemrFjx+bJJ5/MhhtumOuvvz633XZbvvrVryZJbrzxxvTp0ydPPvlkdtppp9x///2ZPHlyHnzwwXTp0iXbbrttLr744px11lkZPXp0WrZsudyxqDACAAB8AWbPnt1gmT9//uceU1dXl9tvvz1z587NgAED8txzz2XhwoUZNGhQ/T5bbLFFNtpoo0yYMCFJMmHChPTr1y9dunSp32efffbJ7Nmz66uUy0vCCAAAVLRP2kILZVw+iaN79+7p0KFD/TJmzJhlxjxp0qS0a9cu1dXVGT58eO6888707ds3tbW1admyZTp27Nhg/y5duqS2tjZJUltb2yBZXLJ9ybbG0JIKAADwBZg2bVpqamrq31dXVy9z38033zwTJ07MrFmz8utf/zrDhg3Lo48++kWE2YCEEQAA4AuwZNbT5dGyZctsttlmSZL+/fvnmWeeyVVXXZXDDjssCxYsyMyZMxtUGWfMmJGuXbsmSbp27Zqnn366wfmWzKK6ZJ/lpSUVAACobOWeIXUlPIZx8eLFmT9/fvr375+11147Dz30UP221157LVOnTs2AAQOSJAMGDMikSZPy3nvv1e/zwAMPpKamJn379m3UuCqMAAAAq5Gzzz47++23XzbaaKN8+OGHue222/LII49k3Lhx6dChQ4499tiMGjUqnTp1Sk1NTU4++eQMGDAgO+20U5Jk7733Tt++fXPUUUflsssuS21tbc4999yMGDHiX7bBliJhBAAAWI289957OfroozN9+vR06NAhW2+9dcaNG5e99torSXLFFVekqqoqQ4YMyfz587PPPvvkpz/9af3xLVq0yN13350TTzwxAwYMSNu2bTNs2LBcdNFFjY5FwggAAFS0JbOVlnP8xrj++uv/5fZWrVrl2muvzbXXXrvMfXr06JF77723UeOW4h5GAAAASlJhBAAAKlr95DNlHL+5UmEEAACgJAkjAAAAJWlJBQAAKlpVVSFVVeXrCy2WcewVpcIIAABASRJGAAAAStKSCgAAVDSzpDadCiMAAAAlqTACAAAVrVAopFDGMl85x15RKowAAACUJGEEAACgJC2pAABARTPpTdOpMAIAAFCShBEAAICStKQCAAAVzSypTafCCAAAQEkqjAAAQEVTYWw6FUYAAABKkjACAABQkpZUAACgonkOY9OpMAIAAFCShBEAAICStKQCAAAVrZAyz5Ka5tuTqsIIAABASRJGAAAAStKSCgAAVDSzpDadCiMAAAAlqTACAAAVrVAo86Q3zbjEqMIIAABASRJGAAAAStKSCgAAVDST3jSdCiMAAAAlSRgBAAAoSUsqAABQ0cyS2nQqjAAAAJSkwggAAFQ0k940nQojAAAAJUkYAQAAKElLKgAAUNFMetN0KowAAACUJGEEAACgJC2pAABAZSvzLKlpvh2pEsYvytRHfpSamppyhwH8k3tfmV7uEIAS9uzdudwhAJ8xe3a5I6BctKQCAABQkgojAABQ0cyS2nQqjAAAAJSkwggAAFS0QpknvWnGBUYVRgAAAEqTMAIAAFCSllQAAKCimfSm6VQYAQAAKEnCCAAAQElaUgEAgIpmltSmU2EEAACgJBVGAACgopn0pulUGAEAAChJwggAAEBJWlIBAICKpiW16VQYAQAAKEnCCAAAQElaUgEAgIrmOYxNp8IIAABASSqMAABARTPpTdOpMAIAAFCShBEAAICStKQCAAAVzaQ3TafCCAAAQEkSRgAAAErSkgoAAFQ0s6Q2nQojAAAAJUkYAQAAKElLKgAAUNEKKfMsqeUbeoWpMAIAAFCSCiMAAFDRqgqFVJWxxFjOsVeUCiMAAAAlSRgBAAAoSUsqAABQ0QqFMk9603w7UlUYAQAAKE3CCAAAQElaUgEAgIpWKBRSKGNfaDnHXlEqjAAAAJQkYQQAACpaVaH8S2OMGTMmX/7yl9O+fft07tw5Bx98cF577bUG++y+++71ldMly/DhwxvsM3Xq1AwePDht2rRJ586dc8YZZ2TRokWNikVLKgAAwGrk0UcfzYgRI/LlL385ixYtyve///3svffemTx5ctq2bVu/3/HHH5+LLrqo/n2bNm3qf66rq8vgwYPTtWvXPPHEE5k+fXqOPvrorL322vnBD36w3LFIGAEAAFYj9913X4P3N910Uzp37pznnnsuAwcOrF/fpk2bdO3ateQ57r///kyePDkPPvhgunTpkm233TYXX3xxzjrrrIwePTotW7Zcrli0pAIAAJWtkKXaN7/IJZ+2pM6ePbvBMn/+/OUKf9asWUmSTp06NVh/6623Zr311stWW22Vs88+Ox999FH9tgkTJqRfv37p0qVL/bp99tkns2fPziuvvLLcX50KIwAAwBege/fuDd5fcMEFGT169L88ZvHixTnllFOy8847Z6uttqpff+SRR6ZHjx7p1q1bXnrppZx11ll57bXXcscddyRJamtrGySLSerf19bWLnfMEkYAAIAvwLRp01JTU1P/vrq6+nOPGTFiRF5++eX86U9/arD+hBNOqP+5X79+2WCDDbLnnntmypQp2XTTTVdazFpSAQCAilYolH9JkpqamgbL5yWMI0eOzN13352HH344G2644b/cd8cdd0ySvPHGG0mSrl27ZsaMGQ32WfJ+Wfc9liJhBAAAWI0Ui8WMHDkyd955Z8aPH59NNtnkc4+ZOHFikmSDDTZIkgwYMCCTJk3Ke++9V7/PAw88kJqamvTt23e5Y9GSCgAAsBoZMWJEbrvttvz2t79N+/bt6+857NChQ1q3bp0pU6bktttuy/7775911103L730Uk499dQMHDgwW2+9dZJk7733Tt++fXPUUUflsssuS21tbc4999yMGDFiuVphl5AwAgAAFa3w6auc4zfG2LFjkyS77757g/U33nhjjjnmmLRs2TIPPvhgrrzyysydOzfdu3fPkCFDcu6559bv26JFi9x999058cQTM2DAgLRt2zbDhg1r8NzG5SFhBAAAWI0Ui8V/ub179+559NFHP/c8PXr0yL333rtCsUgYAQCAilZV+GQp5/jNlUlvAAAAKEnCCAAAQElaUgEAgIpWKBRSKJRx0psyjr2iVBgBAAAoScIIAABASVpSAQCAilYofLKUc/zmSoURAACAklQYAQCAilZVKKSqjGW+co69olQYAQAAKEnCCAAAQElaUgEAgIpm0pumU2EEAACgJAkjAAAAJWlJBQAAKlqhUEihjH2h5Rx7RakwAgAAUJIKIwAAUNFMetN0KowAAACUJGEEAACgJC2pAABARasqFFJVxr7Qco69olQYAQAAKEnCCAAAQElaUgEAgIpW+HQp5/jNlQojAAAAJUkYAQAAKElLKgAAUNEKhUIKZZyptJxjrygVRgAAAEpSYQQAACpaVeGTpZzjN1cqjAAAAJQkYQQAAKAkLakAAEBFM+lN06kwAgAAUNJyVRhfeuml5T7h1ltv3eRgAAAAWH0sV8K47bbbplAopFgslty+ZFuhUEhdXd1KDRAAAGBFNeOu0LJaroTxrbfeWtVxAAAAsJpZroSxR48eqzoOAACAVcKkN03XpElvbrnlluy8887p1q1b3nnnnSTJlVdemd/+9rcrNTgAAADKp9EJ49ixYzNq1Kjsv//+mTlzZv09ix07dsyVV165suMDAACgTBqdMF5zzTX52c9+lnPOOSctWrSoX7/99ttn0qRJKzU4AACAFVVVKP/SXDU6YXzrrbey3XbbLbW+uro6c+fOXSlBAQAAUH6NThg32WSTTJw4can19913X/r06bMyYgIAAGA1sFyzpP6zUaNGZcSIEZk3b16KxWKefvrp/M///E/GjBmT//qv/1oVMQIAADSZWVKbrtEJ43HHHZfWrVvn3HPPzUcffZQjjzwy3bp1y1VXXZXDDz98VcQIAABAGTQ6YUySoUOHZujQofnoo48yZ86cdO7ceWXHBQAAQJk1KWFMkvfeey+vvfZakk9KrOuvv/5KCwoAAGBlKXy6lHP85qrRk958+OGHOeqoo9KtW7fstttu2W233dKtW7d885vfzKxZs1ZFjAAAAJRBoxPG4447Lk899VTuueeezJw5MzNnzszdd9+dZ599Nt/+9rdXRYwAAABNVlUolH1prhrdknr33Xdn3Lhx2WWXXerX7bPPPvnZz36Wfffdd6UGBwAAQPk0usK47rrrpkOHDkut79ChQ9ZZZ52VEhQAAADl1+iE8dxzz82oUaNSW1tbv662tjZnnHFGzjvvvJUaHAAAwIoqFMq/NFfL1ZK63XbbNXjY5Ouvv56NNtooG220UZJk6tSpqa6uzt/+9jf3MQIAAFSI5UoYDz744FUcBgAAAKub5UoYL7jgglUdBwAAwCpRKBQadEyWY/zmqtH3MAIAALBmaPRjNerq6nLFFVfkl7/8ZaZOnZoFCxY02P7BBx+stOAAAABWVLknnmnGBcbGVxgvvPDC/OQnP8lhhx2WWbNmZdSoUTnkkENSVVWV0aNHr4IQAQAAKIdGJ4y33nprfvazn+W0007LWmutlSOOOCL/9V//lfPPPz9PPvnkqogRAACAMmh0wlhbW5t+/folSdq1a5dZs2YlSb72ta/lnnvuWbnRAQAArKCqQqHsS3PV6IRxww03zPTp05Mkm266ae6///4kyTPPPJPq6uqVGx0AAABl0+iE8etf/3oeeuihJMnJJ5+c8847L7169crRRx+df//3f1/pAQIAAFAejZ4l9Yc//GH9z4cddlh69OiRJ554Ir169coBBxywUoNbolAo5M4778zBBx+8Ss4PAABULrOkNt0KP4dxp512yqhRo7LjjjvmBz/4QaOPr62tzcknn5yePXumuro63bt3zwEHHFBfxSy3O+64I3vvvXfWXXfdFAqFTJw4sdwhAQAAfCFWOGFcYvr06TnvvPMadczbb7+d/v37Z/z48bn88sszadKk3Hfffdljjz0yYsSIlRXaCpk7d2522WWX/N//+3/LHQoAAMAXaqUljE1x0kknpVAo5Omnn86QIUPSu3fvbLnllhk1atS/fETHWWedld69e6dNmzbp2bNnzjvvvCxcuLB++4svvpg99tgj7du3T01NTfr3759nn302SfLOO+/kgAMOyDrrrJO2bdtmyy23zL333rvMsY466qicf/75GTRo0Mr74AAAwBemUCiUfWmuGn0P48rywQcf5L777sull16atm3bLrW9Y8eOyzy2ffv2uemmm9KtW7dMmjQpxx9/fNq3b58zzzwzSTJ06NBst912GTt2bFq0aJGJEydm7bXXTpKMGDEiCxYsyGOPPZa2bdtm8uTJadeu3Ur7XPPnz8/8+fPr38+ePXulnRsAAOCLVLaE8Y033kixWMwWW2zR6GPPPffc+p833njjnH766bn99tvrE8apU6fmjDPOqD93r1696vefOnVqhgwZUv8syZ49e67Ix1jKmDFjcuGFF67UcwIAAE1XlfK2Vpa1rXMFLXfCOGrUqH+5/W9/+1ujBi4Wi43a/5/94he/yNVXX50pU6Zkzpw5WbRoUWpqauq3jxo1Kscdd1xuueWWDBo0KP/n//yfbLrppkmS73znOznxxBNz//33Z9CgQRkyZEi23nrrJsfyWWeffXaD72r27Nnp3r37Sjs/AADAF2W5k90XXnjhXy5/+ctfMnDgwOUeuFevXikUCvnzn//cqIAnTJiQoUOHZv/998/dd9+dF154Ieecc04WLFhQv8/o0aPzyiuvZPDgwRk/fnz69u2bO++8M0ly3HHH5c0338xRRx2VSZMmZfvtt88111zTqBj+lerq6tTU1DRYAAAAmqPlrjA+/PDDK3XgTp06ZZ999sm1116b73znO0vdxzhz5syS9zE+8cQT6dGjR84555z6de+8885S+/Xu3Tu9e/fOqaeemiOOOCI33nhjvv71rydJunfvnuHDh2f48OE5++yz87Of/Swnn3zySv18AADA6qHcE88050lvytpOe+2116auri477LBDfvOb3+T111/Pq6++mquvvjoDBgwoeUyvXr0yderU3H777ZkyZUquvvrq+uphknz88ccZOXJkHnnkkbzzzjt5/PHH88wzz6RPnz5JklNOOSXjxo3LW2+9leeffz4PP/xw/bZSPvjgg0ycODGTJ09Okrz22muZOHFiamtrV+I3AQAAsPopa8LYs2fPPP/889ljjz1y2mmnZauttspee+2Vhx56KGPHji15zIEHHphTTz01I0eOzLbbbpsnnniiwfMfW7Rokffffz9HH310evfunUMPPTT77bdf/UQ0dXV1GTFiRPr06ZN99903vXv3zk9/+tNlxvi73/0u2223XQYPHpwkOfzww7PddtvluuuuW4nfBAAAwOqnUFyR2Wf4XLNnz06HDh0y4/1Z7meE1cy9r0wvdwhACXv27lzuEIDPmD17djbq2imzZjWvv2mX/C0+/LZnUt1m5T1Kr7HmfzQn1x355Wb3/SXNe4ZXAAAAVqGyPYcRAADgi1BV+GQp5/jNVZMqjH/84x/zzW9+MwMGDMhf//rXJMktt9ySP/3pTys1OAAAAMqn0Qnjb37zm+yzzz5p3bp1XnjhhcyfPz9JMmvWrPzgBz9Y6QECAABQHo1OGC+55JJcd911+dnPfpa11167fv3OO++c559/fqUGBwAAsKKWPIexnEtz1eiE8bXXXsvAgQOXWt+hQ4fMnDlzZcQEAADAaqDRCWPXrl3zxhtvLLX+T3/6U3r27LlSggIAAKD8Gj1L6vHHH5/vfve7ueGGG1IoFPLuu+9mwoQJOf3003PeeeetihgBAACazCypTdfohPF73/teFi9enD333DMfffRRBg4cmOrq6px++uk5+eSTV0WMAAAAlEGjE8ZCoZBzzjknZ5xxRt54443MmTMnffv2Tbt27VZFfAAAACukUPhkKef4zVWTnsOYJC1btkzfvn2zww47SBYBAABWkjFjxuTLX/5y2rdvn86dO+fggw/Oa6+91mCfefPmZcSIEVl33XXTrl27DBkyJDNmzGiwz9SpUzN48OC0adMmnTt3zhlnnJFFixY1KpZGVxj32GOPfzkt7Pjx4xt7SgAAAD716KOPZsSIEfnyl7+cRYsW5fvf/3723nvvTJ48OW3btk2SnHrqqbnnnnvyq1/9Kh06dMjIkSNzyCGH5PHHH0+S1NXVZfDgwenatWueeOKJTJ8+PUcffXTWXnvt/OAHP1juWBqdMG677bYN3i9cuDATJ07Myy+/nGHDhjX2dAAAAKtUVaGQqjL2hTZ27Pvuu6/B+5tuuimdO3fOc889l4EDB2bWrFm5/vrrc9ttt+WrX/1qkuTGG29Mnz598uSTT2annXbK/fffn8mTJ+fBBx9Mly5dsu222+biiy/OWWedldGjR6dly5bLFUujE8Yrrrii5PrRo0dnzpw5jT0dAADAGmH27NkN3ldXV6e6uvpzj5s1a1aSpFOnTkmS5557LgsXLsygQYPq99liiy2y0UYbZcKECdlpp50yYcKE9OvXL126dKnfZ5999smJJ56YV155Jdttt91yxdzkexg/65vf/GZuuOGGlXU6AACAitK9e/d06NChfhkzZsznHrN48eKccsop2XnnnbPVVlslSWpra9OyZct07Nixwb5dunRJbW1t/T7/nCwu2b5k2/JqdIVxWSZMmJBWrVqtrNMBAACsFFVZiZWyJo6fJNOmTUtNTU39+uWpLo4YMSIvv/xy/vSnP62i6P61RieMhxxySIP3xWIx06dPz7PPPpvzzjtvpQUGAABQSWpqahokjJ9n5MiRufvuu/PYY49lww03rF/ftWvXLFiwIDNnzmxQZZwxY0a6du1av8/TTz/d4HxLZlFdss/yaHSi/c8l1A4dOqRTp07Zfffdc++99+aCCy5o7OkAAAD4J8ViMSNHjsydd96Z8ePHZ5NNNmmwvX///ll77bXz0EMP1a977bXXMnXq1AwYMCBJMmDAgEyaNCnvvfde/T4PPPBAampq0rdv3+WOpVEVxrq6unzrW99Kv379ss466zTmUAAAgLIoFD5Zyjl+Y4wYMSK33XZbfvvb36Z9+/b19xx26NAhrVu3TocOHXLsscdm1KhR6dSpU2pqanLyySdnwIAB2WmnnZIke++9d/r27Zujjjoql112WWpra3PuuedmxIgRy9UKu0SjKowtWrTI3nvvnZkzZzbmMAAAAJbT2LFjM2vWrOy+++7ZYIMN6pdf/OIX9ftcccUV+drXvpYhQ4Zk4MCB6dq1a+6444767S1atMjdd9+dFi1aZMCAAfnmN7+Zo48+OhdddFGjYmn0PYxbbbVV3nzzzaXKogAAAKujqpT5OYxp3NjFYvFz92nVqlWuvfbaXHvttcvcp0ePHrn33nsbNfZnNfoexksuuSSnn3567r777kyfPj2zZ89usAAAAFAZlrvCeNFFF+W0007L/vvvnyQ58MADU/inLL1YLKZQKKSurm7lRwkAAMAXbrkTxgsvvDDDhw/Pww8/vCrjAQAAWKma26Q3q5PlThiX9NHutttuqywYAAAAVh+Nuoex0JxTYwAAABqlUbOk9u7d+3OTxg8++GCFAgIAAFiZqgqfLOUcv7lqVMJ44YUXpkOHDqsqFgAAAFYjjUoYDz/88HTu3HlVxQIAALDSFQop63MYm/Odfct9D6P7FwEAANYsy50wLpklFQAAgDXDcrekLl68eFXGAQAAsEp4DmPTNeqxGgAAAKw5JIwAAACU1KhZUgEAAJobz2FsOhVGAAAASpIwAgAAUJKWVAAAoKIVPn2Vc/zmSoURAACAklQYAQCAimbSm6ZTYQQAAKAkCSMAAAAlaUkFAAAqmpbUplNhBAAAoCQJIwAAACVpSQUAACpaoVBIoVDG5zCWcewVpcIIAABASSqMAABARTPpTdOpMAIAAFCShBEAAICStKQCAAAVrVD4ZCnn+M2VCiMAAAAlSRgBAAAoSUsqAABQ0aoKhVSVsS+0nGOvKBVGAAAASlJhBAAAKprnMDadCiMAAAAlSRgBAAAoSUsqAABQ2cr8HMZoSQUAAKDSSBgBAAAoSUsqAABQ0apSSFUZ+0LLOfaKUmEEAACgJAkjAAAAJWlJBQAAKlqhzLOklnWG1hWkwggAAEBJKowAAEBFqyp8spRz/OZKhREAAICSJIwAAACUpCUVAACoaFWFQqrKOPNMOcdeUSqMAAAAlCRhBAAAoCQtqQAAQEXzHMamU2EEAACgJBVGAACgolWlzJPepPmWGFUYAQAAKEnCCAAAQElaUgEAgIpm0pumU2EEAACgJAkjAAAAJWlJBQAAKlpVylspa85VuuYcOwAAAKuQhBEAAICStKQCAAAVrVAopFDGqUrLOfaKUmEEAACgJBVGAACgohU+Xco5fnOlwggAAEBJEkYAAABK0pIKAABUtKpCIVVlnHimnGOvKBVGAAAASpIwAgAAUJKWVAAAoOI136bQ8lJhBAAAoCQVRgAAoKIVCp8s5Ry/uVJhBAAAoCQJIwAAACVpSQUAACpaoVBIoYx9oeUce0WpMAIAAFCShBEAAICStKQCAAAVrSrlrZQ15ypdc44dAACAVUiFEQAAqGgmvWk6FUYAAIDVzGOPPZYDDjgg3bp1S6FQyF133dVg+zHHHFOfCC9Z9t133wb7fPDBBxk6dGhqamrSsWPHHHvssZkzZ06j4pAwAgAArGbmzp2bbbbZJtdee+0y99l3330zffr0+uV//ud/GmwfOnRoXnnllTzwwAO5++6789hjj+WEE05oVBxaUgEAgIpW+HQp5/iNtd9++2W//fb7l/tUV1ena9euJbe9+uqrue+++/LMM89k++23T5Jcc8012X///fOjH/0o3bp1W644VBgBAAC+ALNnz26wzJ8/f4XO98gjj6Rz587ZfPPNc+KJJ+b999+v3zZhwoR07NixPllMkkGDBqWqqipPPfXUco8hYQQAAPgCdO/ePR06dKhfxowZ0+Rz7bvvvvn5z3+ehx56KP/3//7fPProo9lvv/1SV1eXJKmtrU3nzp0bHLPWWmulU6dOqa2tXe5xtKQCAAAVbXWZJXXatGmpqampX19dXd3kcx5++OH1P/fr1y9bb711Nt100zzyyCPZc889mx7sZ6gwAgAAfAFqamoaLCuSMH5Wz549s9566+WNN95IknTt2jXvvfdeg30WLVqUDz74YJn3PZaiwvgFefWvs9NudrmjAP7ZZp3alTsEoIR9r/5TuUMAPmPRvLnlDoHP8Ze//CXvv/9+NthggyTJgAEDMnPmzDz33HPp379/kmT8+PFZvHhxdtxxx+U+r4QRAACoaFUpb2tlU8aeM2dOfbUwSd56661MnDgxnTp1SqdOnXLhhRdmyJAh6dq1a6ZMmZIzzzwzm222WfbZZ58kSZ8+fbLvvvvm+OOPz3XXXZeFCxdm5MiROfzww5d7htSmxg4AAMAq9Oyzz2a77bbLdtttlyQZNWpUtttuu5x//vlp0aJFXnrppRx44IHp3bt3jj322PTv3z9//OMfG7S53nrrrdliiy2y5557Zv/9988uu+yS//zP/2xUHCqMAABARVtdJr1pjN133z3FYnGZ28eNG/e55+jUqVNuu+22Ro/9z1QYAQAAKEnCCAAAQElaUgEAgIpW+HQp5/jNlQojAAAAJUkYAQAAKElLKgAAUNEKhU+Wco7fXKkwAgAAUJIKIwAAUNGqUkhVGaeeKefYK0qFEQAAgJIkjAAAAJSkJRUAAKhoJr1pOhVGAAAASpIwAgAAUJKWVAAAoKIVPn2Vc/zmSoURAACAkiSMAAAAlKQlFQAAqGhmSW06FUYAAABKUmEEAAAqWiGFVJn0pklUGAEAAChJwggAAEBJWlIBAICKZtKbplNhBAAAoCQJIwAAACVpSQUAACqaltSmU2EEAACgJBVGAACgohU+fZVz/OZKhREAAICSJIwAAACUpCUVAACoaFWFT5Zyjt9cqTACAABQkoQRAACAkrSkAgAAFc0sqU2nwggAAEBJKowAAEBFKxQ+Wco5fnOlwggAAEBJEkYAAABK0pIKAABUtELKO/FMM+5IVWEEAACgNAkjAAAAJWlJBQAAKlpV4ZOlnOM3VyqMAAAAlCRhBAAAoCQtqQAAQEUrfPoq5/jNlQojAAAAJakwAgAAFa1Q+GQp5/jNlQojAAAAJUkYAQAAKElLKgAAUNEKny7lHL+5UmEEAACgJAkjAAAAJWlJBQAAKlpVCqkq41SlVc24KVWFEQAAgJJUGAEAgIpm0pumU2EEAACgJAkjAAAAJWlJBQAAKpue1CZTYQQAAKAkCSMAAAAlaUkFAAAqWuHTVznHb65UGAEAAChJwggAAEBJWlIBAIDKVkgKZkltEhVGAAAASlJhBAAAKprHMDadCiMAAAAlSRgBAAAoSUsqAABQ2fSkNpkKIwAAACVJGAEAAChJSyoAAFDRCp++yjl+c6XCCAAAQEkqjAAAQEUrFD5Zyjl+c6XCCAAAQEkSRgAAAErSkgoAAFQ0j2FsOhVGAAAASpIwAgAAUJKWVAAAoLLpSW0yFUYAAABKUmEEAAAqWuHTVznHb65UGAEAAChJwggAALCaeeyxx3LAAQekW7duKRQKueuuuxpsLxaLOf/887PBBhukdevWGTRoUF5//fUG+3zwwQcZOnRoampq0rFjxxx77LGZM2dOo+KQMAIAABWtUCj/0lhz587NNttsk2uvvbbk9ssuuyxXX311rrvuujz11FNp27Zt9tlnn8ybN69+n6FDh+aVV17JAw88kLvvvjuPPfZYTjjhhEbF4R5GAACA1cx+++2X/fbbr+S2YrGYK6+8Mueee24OOuigJMnPf/7zdOnSJXfddVcOP/zwvPrqq7nvvvvyzDPPZPvtt0+SXHPNNdl///3zox/9KN26dVuuOFQYAQAAvgCzZ89usMyfP79J53nrrbdSW1ubQYMG1a/r0KFDdtxxx0yYMCFJMmHChHTs2LE+WUySQYMGpaqqKk899dRyjyVhBAAAKlphNViSpHv37unQoUP9MmbMmCZ9ntra2iRJly5dGqzv0qVL/bba2tp07ty5wfa11lornTp1qt9neWhJBQAA+AJMmzYtNTU19e+rq6vLGM3yUWEEAAD4AtTU1DRYmpowdu3aNUkyY8aMButnzJhRv61r16557733GmxftGhRPvjgg/p9loeEEQAAqGzl7kdtwiyp/8omm2ySrl275qGHHqpfN3v27Dz11FMZMGBAkmTAgAGZOXNmnnvuufp9xo8fn8WLF2fHHXdc7rG0pAIAAKxm5syZkzfeeKP+/VtvvZWJEyemU6dO2WijjXLKKafkkksuSa9evbLJJpvkvPPOS7du3XLwwQcnSfr06ZN99903xx9/fK677rosXLgwI0eOzOGHH77cM6QmEkYAAKDCFT59lXP8xnr22Wezxx571L8fNWpUkmTYsGG56aabcuaZZ2bu3Lk54YQTMnPmzOyyyy6577770qpVq/pjbr311owcOTJ77rlnqqqqMmTIkFx99dWNikPCCAAAsJrZfffdUywWl7m9UCjkoosuykUXXbTMfTp16pTbbrttheJwDyMAAAAlqTACAAAVrVD4ZCnn+M2VCiMAAAAlSRgBAAAoSUsqAABQ0VbBoxAbPX5zpcIIAABASSqMAABAZVNibDIVRgAAAEqSMAIAAFCSllQAAKCiFT59lXP85kqFEQAAgJIkjAAAAJSkJRUAAKhohcInSznHb66aRYWxUCjkrrvuKncYAAAAa5SyJ4y1tbU5+eST07Nnz1RXV6d79+454IAD8tBDD5U7tCxcuDBnnXVW+vXrl7Zt26Zbt245+uij8+6775Y7NAAAgFWurC2pb7/9dnbeeed07Ngxl19+efr165eFCxdm3LhxGTFiRP785z+XM7x89NFHef7553Peeedlm222yT/+8Y9897vfzYEHHphnn322rLEBAADLp/DpUs7xm6uyVhhPOumkFAqFPP300xkyZEh69+6dLbfcMqNGjcqTTz65zOPOOuus9O7dO23atEnPnj1z3nnnZeHChfXbX3zxxeyxxx5p3759ampq0r9///oE75133skBBxyQddZZJ23bts2WW26Ze++9t+Q4HTp0yAMPPJBDDz00m2++eXbaaaf8x3/8R5577rlMnTp15X4ZAAAAq5myVRg/+OCD3Hfffbn00kvTtm3bpbZ37Nhxmce2b98+N910U7p165ZJkybl+OOPT/v27XPmmWcmSYYOHZrtttsuY8eOTYsWLTJx4sSsvfbaSZIRI0ZkwYIFeeyxx9K2bdtMnjw57dq1W+64Z82alUKhsMz45s+fn/nz59e/nz179nKfGwAAWAWUGJusbAnjG2+8kWKxmC222KLRx5577rn1P2+88cY5/fTTc/vtt9cnjFOnTs0ZZ5xRf+5evXrV7z916tQMGTIk/fr1S5L07NlzucedN29ezjrrrBxxxBGpqakpuc+YMWNy4YUXNvozAQAArG7K1pJaLBabfOwvfvGL7LzzzunatWvatWuXc889t0GL6KhRo3Lcccdl0KBB+eEPf5gpU6bUb/vOd76TSy65JDvvvHMuuOCCvPTSS8s15sKFC3PooYemWCxm7Nixy9zv7LPPzqxZs+qXadOmNflzAgAAlFPZEsZevXqlUCg0emKbCRMmZOjQodl///1z991354UXXsg555yTBQsW1O8zevTovPLKKxk8eHDGjx+fvn375s4770ySHHfccXnzzTdz1FFHZdKkSdl+++1zzTXX/MsxlySL77zzTh544IFlVheTpLq6OjU1NQ0WAACgfAqrwau5KlvC2KlTp+yzzz659tprM3fu3KW2z5w5s+RxTzzxRHr06JFzzjkn22+/fXr16pV33nlnqf169+6dU089Nffff38OOeSQ3HjjjfXbunfvnuHDh+eOO+7Iaaedlp/97GfLjHNJsvj666/nwQcfzLrrrtv4DwsAANAMlXWW1GuvvTZ1dXXZYYcd8pvf/Cavv/56Xn311Vx99dUZMGBAyWN69eqVqVOn5vbbb8+UKVNy9dVX11cPk+Tjjz/OyJEj88gjj+Sdd97J448/nmeeeSZ9+vRJkpxyyikZN25c3nrrrTz//PN5+OGH67d91sKFC/ONb3wjzz77bG699dbU1dWltrY2tbW1DSqaAAAAlaisz2Hs2bNnnn/++Vx66aU57bTTMn369Ky//vrp37//Mu8TPPDAA3Pqqadm5MiRmT9/fgYPHpzzzjsvo0ePTpK0aNEi77//fo4++ujMmDEj6623Xg455JD6iWjq6uoyYsSI/OUvf0lNTU323XffXHHFFSXH+utf/5rf/e53SZJtt922wbaHH344u++++0r5HgAAgFWnUPhkKef4zVWhuCKzz/C5Zs+enQ4dOuSRl6alXXv3M8LqZO0Wzfhfb6hg377t+XKHAHzGonlz8+zowZk1a1azmqNjyd/iz/7v9LL+LT7nw9nZvvcGze77S8pcYQQAAFjVPIax6cp6DyMAAACrLwkjAAAAJWlJBQAAKpue1CZTYQQAAKAkCSMAAAAlaUkFAAAqWuHTVznHb65UGAEAAChJwggAAEBJWlIBAIDKVkgKZkltEhVGAAAASlJhBAAAKprHMDadCiMAAAAlSRgBAAAoSUsqAABQ2fSkNpkKIwAAACVJGAEAAChJSyoAAFDRCp++yjl+c6XCCAAAQEkqjAAAQEUrFD5Zyjl+c6XCCAAAQEkSRgAAAErSkgoAAFQ0j2FsOhVGAAAASpIwAgAAUJKWVAAAoLLpSW0yFUYAAABKUmEEAAAqWuHTVznHb65UGAEAAChJwggAAEBJWlIBAICKVkhSKGNXaPNtSFVhBAAAYBkkjAAAAJSkJRUAAKhoHsPYdCqMAAAAlCRhBAAAoCQtqQAAQEUrFMo8S2oz7klVYQQAAKAkFUYAAKDCmfamqVQYAQAAKEnCCAAAQElaUgEAgIpm0pumU2EEAACgJAkjAAAAJWlJBQAAKpo5UptOhREAAICSVBgBAICKZtKbplNhBAAAoCQJIwAAACVpSQUAACpa4dNXOcdvrlQYAQAAKEnCCAAAQElaUgEAgMrmQYxNpsIIAABASRJGAAAAStKSCgAAVDQdqU2nwggAAEBJEkYAAKCiFQrlXxpj9OjRKRQKDZYtttiifvu8efMyYsSIrLvuumnXrl2GDBmSGTNmrORv7RMSRgAAgNXMlltumenTp9cvf/rTn+q3nXrqqfn973+fX/3qV3n00Ufz7rvv5pBDDlklcbiHEQAA4Aswe/bsBu+rq6tTXV1dct+11lorXbt2XWr9rFmzcv311+e2227LV7/61STJjTfemD59+uTJJ5/MTjvttFJjVmEEAAAqWmE1eCVJ9+7d06FDh/plzJgxy4z59ddfT7du3dKzZ88MHTo0U6dOTZI899xzWbhwYQYNGlS/7xZbbJGNNtooEyZMWOnfnQojAADAF2DatGmpqampf7+s6uKOO+6Ym266KZtvvnmmT5+eCy+8MLvuumtefvnl1NbWpmXLlunYsWODY7p06ZLa2tqVHrOEEQAA4AtQU1PTIGFclv3226/+56233jo77rhjevTokV/+8pdp3br1qgxxKVpSAQCAylZYDZYV0LFjx/Tu3TtvvPFGunbtmgULFmTmzJkN9pkxY0bJex5XlIQRAABgNTZnzpxMmTIlG2ywQfr375+11147Dz30UP321157LVOnTs2AAQNW+thaUgEAgIq2Eop8Kzx+Y5x++uk54IAD0qNHj7z77ru54IIL0qJFixxxxBHp0KFDjj322IwaNSqdOnVKTU1NTj755AwYMGClz5CaSBgBAABWK3/5y19yxBFH5P3338/666+fXXbZJU8++WTWX3/9JMkVV1yRqqqqDBkyJPPnz88+++yTn/70p6skFgkjAADAauT222//l9tbtWqVa6+9Ntdee+0qj0XCCAAAVLRC4ZOlnOM3Vya9AQAAoCQJIwAAACVpSQUAACpcIYVmNU/q6kOFEQAAgJJUGAEAgIpm0pumU2EEAACgJAkjAAAAJUkYAQAAKEnCCAAAQEkSRgAAAEoySyoAAFDRzJLadCqMAAAAlCRhBAAAoCQtqQAAQEUrfPoq5/jNlQojAAAAJakwAgAAFc2kN02nwggAAEBJEkYAAABK0pIKAABUtMKnSznHb65UGAEAAChJwggAAEBJWlIBAIDKpie1yVQYAQAAKEmFEQAAqGiFT1/lHL+5UmEEAACgJAkjAAAAJWlJBQAAKlqh8MlSzvGbKxVGAAAASpIwAgAAUJKWVAAAoKJ5DGPTqTACAABQkoQRAACAkrSkAgAAlU1PapOpMAIAAFCSCiMAAFDRCp++yjl+c6XCCAAAQEkSRgAAAErSkgoAAFS0QuGTpZzjN1cqjAAAAJSkwriKFYvFJMncOR+WORLgs9Zu0Yz/uw8q2KJ5c8sdAvAZdfM+SvL//23b3MyePXuNHn9FSBhXsQ8//CRRHPyVvmWOBAAAVsyHH36YDh06lDuM5dayZct07do1vTbpXu5Q0rVr17Rs2bLcYTRaodhc/5ugmVi8eHHefffdtG/fPoXm3LxMkk/+d6h79+6ZNm1aampqyh0OENclrK5cm5WlWCzmww8/TLdu3VJV1bzuaps3b14WLFhQ7jDSsmXLtGrVqtxhNJoK4ypWVVWVDTfcsNxhsJLV1NT45QerGdclrJ5cm5WjOVUW/1mrVq2aZaK2umhe/z0AAADAF0bCCAAAQEkSRmiE6urqXHDBBamuri53KMCnXJewenJtQmUw6Q0AAAAlqTACAABQkoQRAACAkiSMAAAAlCRhBAAAoCQJIwAAACVJGGElW7x4cblDAEowKTisnvzehNXbWuUOACrJ4sWLU1VVlddeey3XXntt/va3v2WLLbbIkCFDstVWW5U7PFhjLbk2a2trU1VVlc6dO5c7JCD//7X55ptv5vbbb89f/vKXbLPNNvn2t79d7tCAT6kwwkpUVVWVV199NTvssEPefvvtVFdX56c//WlOOumkXH311eUOD9ZIxWKx/trs3r17hg4dmr///e/lDgvWeEuSxUmTJmXXXXfNH//4x/zv//5vTj755Hzve98rd3jApwpFPTqwUhSLxdTV1WX48OFZvHhxbrjhhiTJjBkz8r3vfS+TJ0/OwQcfnLPPPrvMkcKa57333ss3vvGNtGvXLi+//HL69OmTW2+9Neutt165Q4M12tSpU7PXXnvlwAMPzOWXX54kueOOOzJ8+PA89NBD6devX5kjBFQYYSUpFApZa6218ve//z3z589P8kkS2aVLl1x22WXp379/7r777vzyl78sc6Sw5nn55ZezySab5Pzzz8+4ceMyefJklUYos2KxmDvuuCPdu3dv8J+pW2+9daqrq7No0aIyRgcsIWGElWRJhXHDDTfMP/7xj8yZMydJUldXl/XXXz/nn39+WrVqlZtvvrnMkcKap3///jnuuOOy0047pU+fPg2Sxr/97W/1+5l8A744hUIhAwcOzE477ZROnTrVr99ss83SunXrzJgxo4zRAUtIGGElKRQKadGiRY477rg89NBD+fGPf1y/rq6uLl27ds1PfvKT/OEPf8izzz5b7nBhjdKhQ4fsuuuuST5JCvv27Zv7778/kydPzje/+c38/e9/z6JFi/If//Ef+d3vflfmaGHN8aUvfSmXXHJJkoYzGVdVVWXevHn17++99968++67X3h8gFlSYaVavHhxtt122/z0pz/NCSeckNatW+fMM89MixYtkiRrrbVWtthii7Rv377MkcKaq6rqk/8r7dOnT+6///7svffeOeqoo9KlS5f893//d1599dUyRwhrpkKhkEWLFtVPVLXkd+U555yTMWPG5J133ilzhLBmkjDCSrTkD9Gjjz46c+bMyWmnnZZp06Zl6NCh6dGjR2677bbMmzcvHTt2LG+gQJJPksZ77rkn2267bdZZZ508/fTT6dWrV7nDgjVWoVBI8km1sbq6OpdeemmuuuqqPP300+nevXuZo4M1k1lSYRX6/e9/n5NPPjl1dXVp3bp15s+fnzvvvDNf+tKXyh0akGThwoX5zne+k1tuuSVPP/10+vbtW+6QgCQ77rhj5s6dm9dffz2PP/54tt9++3KHBGssFUZogmKxmEKhkHfffTetWrVqcLP+PzvggAOyww475J133sm8efPSq1evbLDBBl9wtLDmWN5rc4lXXnklzz//fB5++GHJIqxCy3ttLl68OHPmzMm0adMyY8aMvPjii9lqq62+4GiBf2bSG2ikJb/0fvvb3+bQQw/Ngw8+mA8//HCZ+3bp0iU77LBDBg4cKFmEVagx1+YSm2++ecaNG5cvf/nLX1CUsOZpzLVZVVWVmpqa3HDDDXn55Zcli7AakDBCIy35pTd06NAccMABGTBgwFKT2Czp9F5yLwaw6jXm2lyidevW7imGVawp1+a+++6bPn36fJFhAsvgHkZopL/+9a/Zd999c/zxx+c73/lOFi5cmPnz5+epp55Kp06dst1225U7RFgjuTZh9eTahObNPYzQSGuttVbatm2bDTfcMO+//35++tOf5sEHH8zLL7+c9dZbLz/4wQ8yZMiQcocJaxzXJqyeXJvQvGlJhc+xpAj/3nvv5aOPPkqrVq1SLBZzzTXXZJNNNskLL7yQIUOG5IEHHki3bt0yadKkMkcMawbXJqyeXJtQWVQY4V9YcqP+73//+1x22WU588wzc8ABB+R//ud/cv/99+fwww/PEUcckZqamiRJu3bt6p/FCKw6rk1YPbk2ofK4hxE+x1133ZWjjjoqZ599dg4//PD07NlzqX0++uijXHzxxfmv//qvPP744+ndu3cZIoU1i2sTVk+uTagsEkb4F/7yl79kr732yvDhw/Pd7343ixYtSl1dXZ5++umst9566dOnT2699db85je/yfPPP58777zTzfvwBXBtwurJtQmVR0sq/AuLFi1K27Zt86UvfSnvvfdebrjhhtx333157rnnss022+Tiiy/OoEGD8vbbb+fyyy/PpptuWu6QYY3g2oTVk2sTKo8KI/wLM2fOzDbbbJPu3bvnz3/+cwYOHJidd945X/nKV3LiiSfmiCOOyFlnnZXFixe7BwO+QK5NWD25NqHyqDDCp5bcqD979uy0adMmH3/8cTp27JgnnngiN910U4488sgcfvjhWWeddVIoFLLhhhtm8eLFST55KDGwarg2YfXk2oQ1gwoj5P//pfeHP/whY8eOTW1tbfr06ZPjjjsuu+66axYtWpS11vrk/1cWLFiQ0aNH19+o36tXrzJHD5XLtQmrJ9cmrDn0ArBGW/L/JYVCIb/97W/zjW98I9tvv32GDRuWjz/+OIcffngee+yxrLXWWikWi/n5z3+er3/967n11lszbtw4v/RgFXFtwurJtQlrHhVG1kh///vfs95669W/f+2113LkkUfm+OOPz/DhwzNjxoz0798/1dXV+cc//pE777wzu+22W6ZOnZr//M//zLBhw/zSg1XAtQmrJ9cmrLlUGFnjXHPNNfnqV7+aV155pX5dsVjMDjvskG9+85uZNm1adt111+y///654447svHGG+ewww7LAw88kI022igXXXSRX3qwCrg2YfXk2oQ1mwoja5zp06dn2223zZZbbpn/+I//SN++fZMk7777brp165YTTzwx77//fm6++ea0bt06Q4cOze9///ust956mTRpUtq0aeNmfVgFXJuwenJtwppNhZE1wpL/F6mrq8sGG2yQF198MX/+858zfPjwvPzyy0mSbt265eOPP86LL76Yvn37pnXr1kmSmpqaXHPNNXn66afTtm1bv/RgJXJtwurJtQksIWGk4i1evDiFQiF/+9vf8sILL+TJJ59M165d88ILL+TNN9/MSSedlMmTJydJWrdunb59++aXv/xlfvnLX+bUU0/NPffck913373BvRvAinNtwurJtQn8My2pVLQlDwaePHlyTjjhhLRv3z5t2rTJrbfemlatWtXfpN+zZ8+MHTs2W265ZZ588slcdtlleeaZZ9KpU6fcdNNN2W677cr9UaCiuDZh9eTaBD5LwkjFWvKMqFdeeSW77LJLTjrppHz729/OhhtumKqqqvpnRC355bfJJpvk+uuvT+/evbNw4cJMnz497dq1S6dOncr9UaCiuDZh9eTaBEqRMFLRPvjggxx00EH50pe+lKuuuqp+/ZJfip/95bfZZpvlmmuuSb9+/coYNVQ+1yasnlybwGe5h5GKVltbm+nTp2fIkCFZvHhx/folN+C3aNEixWIxXbp0ybPPPpsnn3wy3/ve97JgwYJyhQxrBNcmrJ5cm8BnrVXuAGBVmjhxYt55553suuuuKRQK9fdmLFEoFPLRRx/lxRdfzIABAzJ16tTMmjUrLVu2LGPUUPlcm7B6cm0Cn6XCSEXbeOONs9Zaa+WOO+5Ikga/9Ja44YYbcsEFF+Sjjz5K586dPVwYvgCuTVg9uTaBz5IwUtF69OiRmpqa/PznP88777xTv/6fb919++23079///rnRwGrnmsTVk+uTeCzJIxUtH/7t3/L2LFjM27cuJx33nn1z41a0lLz/e9/P7/+9a/zrW99y4OF4Qvk2oTVk2sT+CyzpFLxFi9enJ/97GcZOXJkNttsswwYMCCtWrXKX//61zz55JO57777PC8KysC1Casn1ybwzySMrDGefvrpXH755XnjjTfSvn37fOUrX8mxxx7r3gsoM9cmrJ5cm0AiYWQNU1dXlxYtWpQ7DOAzXJuwenJtAu5hZI3yz7O9+b8SWH24NmH15NoEVBgBAAAoSYURAACAkiSMAAAAlCRhBAAAoCQJIwAAACVJGAEAAChJwggAAEBJEkYAAABKkjACsEodc8wxOfjgg+vf77777jnllFO+8DgeeeSRFAqFzJw5c5WN8dnP2hRfRJwAsLwkjABroGOOOSaFQiGFQiEtW7bMZpttlosuuiiLFi1a5WPfcccdufjii5dr3y86edp4441z5ZVXfiFjAUBzsFa5AwCgPPbdd9/ceOONmT9/fu69996MGDEia6+9ds4+++yl9l2wYEFatmy5Usbt1KnTSjkPALDqqTACrKGqq6vTtWvX9OjRIyeeeGIGDRqU3/3ud0n+/9bKSy+9NN26dcvmm2+eJJk2bVoOPfTQdOzYMZ06dcpBBx2Ut99+u/6cdXV1GTVqVDp27Jh11103Z555ZorFYoNxP9uSOn/+/Jx11lnp3r17qqurs9lmm+X666/P22+/nT322CNJss4666RQKOSYY45JkixevDhjxozJJptsktatW2ebbbbJr3/96wbj3Hvvvendu3dat26dPfbYo0GcTVFXV5djjz22fszNN988V111Vcl9L7zwwqy//vqpqanJ8OHDs2DBgvptyxM7AKwuVBgBSJK0bt0677//fv37hx56KDU1NXnggQeSJAsXLsw+++yTAQMG5I9//GPWWmutXHLJJdl3333z0ksvpWXLlvnxj3+cm266KTfccEP69OmTH//4x7nzzjvz1a9+dZnjHn300ZkwYUKuvvrqbLPNNnnrrbfy97//Pd27d89vfvObDBkyJK+99lpqamrSunXrJMmYMWPy3//937nuuuvSq1evPPbYY/nmN7+Z9ddfP7vttlumTZuWQw45JCNGjMgJJ5yQZ599NqeddtoKfT+LFy/OhhtumF/96ldZd91188QTT+SEE07IBhtskEMPPbTB99aqVas88sgjefvtt/Otb30r6667bi699NLlih0AVitFANY4w4YNKx500EHFYrFYXLx4cfGBBx4oVldXF08//fT67V26dCnOnz+//phbbrmluPnmmxcXL15cv27+/PnF1q1bF8eNG1csFovFDTbYoHjZZZfVb1+4cGFxww03rB+rWCwWd9ttt+J3v/vdYrFYLL722mvFJMUHHnigZJwPP/xwMUnxH//4R/26efPmFdu0aVN84oknGux77LHHFo844ohisVgsnn322cW+ffs22H7WWWctda7P6tGjR/GKK65Y5vbPGjFiRHHIkCH174cNG1bs1KlTce7cufXrxo4dW2zXrl2xrq5uuWIv9ZkBoFxUGAHWUHfffXfatWuXhQsXZvHixTnyyCMzevTo+u39+vVrcN/iiy++mDfeeCPt27dvcJ558+ZlypQpmTVrVqZPn54dd9yxfttaa62V7bfffqm21CUmTpyYFi1aNKqy9sYbb+Sjjz7KXnvt1WD9ggULst122yVJXn311QZxJMmAAQOWe4xlufbaa3PDDTdk6tSp+fjjj7NgwYJsu+22DfbZZptt0qZNmwbjzpkzJ9OmTcucOXM+N3YAWJ1IGAHWUHvssUfGjh2bli1bplu3bllrrYa/Etq2bdvg/Zw5c9K/f//ceuutS51r/fXXb1IMS1pMG2POnDlJknvuuSf/9m//1mBbdXV1k+JYHrfffntOP/30/PjHP86AAQPSvn37XH755XnqqaeW+xzlih0AmkrCCLCGatu2bTbbbLPl3v9LX/pSfvGLX6Rz586pqakpuc8GG2yQp556KgMHDkySLFq0KM8991y+9KUvldy/X79+Wbx4cR599NEMGjRoqe1LKpx1dXX16/r27Zvq6upMnTp1mZXJPn361E/gs8STTz75+R/yX3j88cfzla98JSeddFL9uilTpiy134svvpiPP/64Phl+8skn065du3Tv3j2dOnX63NgBYHVillQAlsvQoUOz3nrr5aCDDsof//jHvPXWW3nkkUfyne98J3/5y1+SJN/97nfzwx/+MHfddVf+/Oc/56STTvqXz1DceOONM2zYsPz7v/977rrrrvpz/vKXv0yS9OjRI4VCIXfffXf+9re/Zc6cOWnfvn1OP/30nHrqqbn55pszZcqUPP/887nmmmty8803J0mGDx+e119/PWeccUZee+213HbbbbnpppuW63P+9a9/zcSJExss//jHP9KrV688++yzGTduXP73f/835513Xp555pmljl+wYEGOPfbYTJ48Offee28uuOCCjBw5MlVVVcsVOwCsTiSMACyXNm3a5LHHHstGG22UQw45JH369Mmxxx6befPm1VccTzvttBx11FEZNmxYfdvm17/+9X953rFjx+Yb3/hGTjrppGyxxRY5/vjjM3fu3CTJv/3bv+XCCy/M9773vXTp0iUjR45Mklx88cU577zzMmbMmPTp0yf77rtv7rnnnmyyySZJko022ii/+c1vctddd2WbbbbJddddlx/84AfL9Tl/9KMfZbvttmuw3HPPPfn2t7+dQw45JIcddlh23HHHvP/++w2qjUvsueee6dWrVwYOHJjDDjssBx54YIN7Qz8vdgBYnRSKy5qJAAAAgDWaCiMAAAAlSRgBAAAoScIIAABASRJGAAAASpIwAgAAUJKEEQAAgJIkjAAAAJQkYQQAAKAkCSMAAAAlSRgBAAAoScIIAABASf8frL4vYdgMx1cAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Roberta\n",
        "#Baseline models\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/Labeled Dataset/long_sentiments.csv'\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "\n",
        "class_counts = df['label'].value_counts().to_dict()\n",
        "total_samples = len(df)\n",
        "class_weights = {\n",
        "    cls: total_samples / count for cls, count in class_counts.items()\n",
        "}\n",
        "weights = torch.tensor([class_weights[i] for i in range(3)], dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "roberta_model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
        "\n",
        "\n",
        "def get_embeddings(sentences, model, tokenizer):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    BATCH_SIZE = 32\n",
        "    for i in range(0, len(sentences), BATCH_SIZE):\n",
        "        batch_sentences = sentences[i:i + BATCH_SIZE]\n",
        "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embeddings.extend(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n",
        "    return np.array(embeddings)\n",
        "\n",
        "parent_embeddings = get_embeddings(df['body_parent'].tolist(), roberta_model, tokenizer)\n",
        "child_embeddings = get_embeddings(df['body_child'].tolist(), roberta_model, tokenizer)\n",
        "\n",
        "all_msg_ids = pd.concat([df['msg_id_parent'], df['msg_id_child']]).unique()\n",
        "msg_to_int = {msg: i for i, msg in enumerate(all_msg_ids)}\n",
        "edges = torch.tensor([(msg_to_int[parent], msg_to_int[child]) for parent, child in zip(df['msg_id_parent'], df['msg_id_child'])], dtype=torch.long).t().contiguous()\n",
        "\n",
        "node_features = torch.cat((torch.tensor(parent_embeddings, dtype=torch.float),\n",
        "                           torch.tensor(child_embeddings, dtype=torch.float),\n",
        "                           torch.tensor(df['sentiment_parent'].values.reshape(-1,1), dtype=torch.float),\n",
        "                           torch.tensor(df['sentiment_child'].values.reshape(-1,1), dtype=torch.float)), dim=1)\n",
        "labels = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "\n",
        "\n",
        "train_end = int(0.7 * total_samples)\n",
        "val_end = train_end + int(0.15 * total_samples)\n",
        "indices = torch.randperm(total_samples)\n",
        "train_indices = indices[:train_end]\n",
        "val_indices = indices[train_end:val_end]\n",
        "test_indices = indices[val_end:]\n",
        "\n",
        "train_mask = torch.zeros(total_samples, dtype=torch.bool).index_fill_(0, train_indices, True)\n",
        "val_mask = torch.zeros(total_samples, dtype=torch.bool).index_fill_(0, val_indices, True)\n",
        "test_mask = torch.zeros(total_samples, dtype=torch.bool).index_fill_(0, test_indices, True)\n",
        "\n",
        "data = Data(x=node_features, edge_index=edges, y=labels, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask).to(device)\n",
        "\n",
        "class GATClassifier(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GATClassifier, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, 16, heads=8, dropout=0.6)\n",
        "        self.conv2 = GATConv(16 * 8, out_channels, heads=1, concat=False, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = F.dropout(data.x, p=0.5, training=self.training)\n",
        "        x = F.elu(self.conv1(x, data.edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, data.edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = GATClassifier(node_features.shape[1], 3).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "\n",
        "# Implement early stopping logic\n",
        "PATIENCE = 20\n",
        "best_val_acc = 0\n",
        "counter = 0\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "def train(data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask], weight=weights)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(data, mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(data).max(dim=1)[1]\n",
        "    correct = preds[mask].eq(data.y[mask]).sum().item()\n",
        "    return correct / mask.sum().item(), preds\n",
        "\n",
        "for epoch in range(200):\n",
        "    loss = train(data)\n",
        "    train_acc = evaluate(data, data.train_mask)[0]\n",
        "    val_acc = evaluate(data, data.val_mask)[0]\n",
        "    print(f\"Epoch: {epoch + 1}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= PATIENCE:\n",
        "            print(\"Early Stopping triggered.\")\n",
        "            break\n",
        "\n",
        "acc, predictions = evaluate(data, data.test_mask)\n",
        "predictions = predictions[data.test_mask].cpu().numpy()\n",
        "ground_truth = data.y[data.test_mask].cpu().numpy()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(ground_truth, predictions))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(ground_truth, predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "classes = [\"Class 0\", \"Class 1\", \"Class 2\"]\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=45)\n",
        "plt.yticks(tick_marks, classes)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "\n",
        "plt.savefig(\"confusion_matrix.jpeg\", format=\"jpeg\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PCtW2V2YPSF",
        "outputId": "383065d6-eba2-4d57-c145-da9b028c79af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Iteration: 0, Loss: 1.1107900142669678\n",
            "Epoch: 0, Iteration: 10, Loss: 1.8481004238128662\n",
            "Epoch: 0, Iteration: 20, Loss: 1.90625\n",
            "Epoch: 0, Iteration: 30, Loss: 1.5911160707473755\n",
            "Epoch: 0, Iteration: 40, Loss: 1.2287205457687378\n",
            "Epoch: 0, Iteration: 50, Loss: 1.1356794834136963\n",
            "Epoch: 0, Iteration: 60, Loss: 0.9695456027984619\n",
            "Epoch: 0, Iteration: 70, Loss: 1.2504440546035767\n",
            "Epoch: 0, Iteration: 80, Loss: 1.1584497690200806\n",
            "Epoch: 0, Iteration: 90, Loss: 1.3733052015304565\n",
            "Epoch: 0, Iteration: 100, Loss: 0.9313027858734131\n",
            "Epoch: 0, Iteration: 110, Loss: 1.2123568058013916\n",
            "Epoch: 0, Iteration: 120, Loss: 1.1374573707580566\n",
            "Epoch: 0, Iteration: 130, Loss: 1.2286139726638794\n",
            "Epoch: 0, Iteration: 140, Loss: 1.0654475688934326\n",
            "Epoch: 0, Iteration: 150, Loss: 0.957149863243103\n",
            "Epoch: 0, Iteration: 160, Loss: 0.8964385986328125\n",
            "Epoch: 0, Iteration: 170, Loss: 1.2857216596603394\n",
            "Epoch: 0, Iteration: 180, Loss: 1.2467820644378662\n",
            "Epoch: 0, Iteration: 190, Loss: 0.8950372338294983\n",
            "Epoch: 0, Iteration: 200, Loss: 1.1325135231018066\n",
            "Epoch: 0, Iteration: 210, Loss: 1.0092897415161133\n",
            "Epoch: 0, Iteration: 220, Loss: 1.0536500215530396\n",
            "Epoch: 0, Iteration: 230, Loss: 1.0243006944656372\n",
            "Epoch: 0, Iteration: 240, Loss: 1.1254384517669678\n",
            "Epoch: 0, Iteration: 250, Loss: 1.0401524305343628\n",
            "Epoch: 0, Iteration: 260, Loss: 1.0761302709579468\n",
            "Epoch: 0, Iteration: 270, Loss: 1.1197476387023926\n",
            "Epoch: 0, Iteration: 280, Loss: 1.1144027709960938\n",
            "Epoch: 0, Iteration: 290, Loss: 1.0967257022857666\n",
            "Epoch: 0, Iteration: 300, Loss: 1.0477956533432007\n",
            "Epoch: 0, Iteration: 310, Loss: 1.1594247817993164\n",
            "Epoch: 0, Iteration: 320, Loss: 1.146906852722168\n",
            "Epoch: 0, Iteration: 330, Loss: 1.0492324829101562\n",
            "Epoch: 0, Iteration: 340, Loss: 1.1716276407241821\n",
            "Epoch: 0, Iteration: 350, Loss: 1.145396113395691\n",
            "Epoch: 0, Iteration: 360, Loss: 1.2153221368789673\n",
            "Epoch: 0, Iteration: 370, Loss: 1.121703028678894\n",
            "Epoch: 0, Iteration: 380, Loss: 1.1200034618377686\n",
            "Epoch: 0, Iteration: 390, Loss: 1.01661217212677\n",
            "Epoch: 0, Iteration: 400, Loss: 0.9789075255393982\n",
            "Epoch: 0, Iteration: 410, Loss: 1.087708830833435\n",
            "Epoch: 0, Iteration: 420, Loss: 1.0678608417510986\n",
            "Epoch: 0, Iteration: 430, Loss: 1.136135220527649\n",
            "Epoch: 0, Iteration: 440, Loss: 1.0783426761627197\n",
            "Epoch: 0, Iteration: 450, Loss: 1.1607717275619507\n",
            "Epoch: 0, Iteration: 460, Loss: 0.9954282641410828\n",
            "Epoch: 0, Iteration: 470, Loss: 1.077309489250183\n",
            "Epoch: 1, Iteration: 0, Loss: 1.1463468074798584\n",
            "Epoch: 1, Iteration: 10, Loss: 1.1490904092788696\n",
            "Epoch: 1, Iteration: 20, Loss: 1.145751953125\n",
            "Epoch: 1, Iteration: 30, Loss: 1.1235541105270386\n",
            "Epoch: 1, Iteration: 40, Loss: 1.060505747795105\n",
            "Epoch: 1, Iteration: 50, Loss: 1.0828056335449219\n",
            "Epoch: 1, Iteration: 60, Loss: 1.0155564546585083\n",
            "Epoch: 1, Iteration: 70, Loss: 1.0957163572311401\n",
            "Epoch: 1, Iteration: 80, Loss: 1.1525102853775024\n",
            "Epoch: 1, Iteration: 90, Loss: 1.1569682359695435\n",
            "Epoch: 1, Iteration: 100, Loss: 1.0221757888793945\n",
            "Epoch: 1, Iteration: 110, Loss: 1.1053075790405273\n",
            "Epoch: 1, Iteration: 120, Loss: 1.0785189867019653\n",
            "Epoch: 1, Iteration: 130, Loss: 1.1212503910064697\n",
            "Epoch: 1, Iteration: 140, Loss: 1.1780081987380981\n",
            "Epoch: 1, Iteration: 150, Loss: 1.0488173961639404\n",
            "Epoch: 1, Iteration: 160, Loss: 1.0717471837997437\n",
            "Epoch: 1, Iteration: 170, Loss: 1.1259194612503052\n",
            "Epoch: 1, Iteration: 180, Loss: 1.0971219539642334\n",
            "Epoch: 1, Iteration: 190, Loss: 0.9766326546669006\n",
            "Epoch: 1, Iteration: 200, Loss: 1.1294397115707397\n",
            "Epoch: 1, Iteration: 210, Loss: 1.0046465396881104\n",
            "Epoch: 1, Iteration: 220, Loss: 1.0040242671966553\n",
            "Epoch: 1, Iteration: 230, Loss: 1.035310983657837\n",
            "Epoch: 1, Iteration: 240, Loss: 1.0004875659942627\n",
            "Epoch: 1, Iteration: 250, Loss: 0.9589331746101379\n",
            "Epoch: 1, Iteration: 260, Loss: 1.0551795959472656\n",
            "Epoch: 1, Iteration: 270, Loss: 1.4172123670578003\n",
            "Epoch: 1, Iteration: 280, Loss: 1.4437332153320312\n",
            "Epoch: 1, Iteration: 290, Loss: 1.0263171195983887\n",
            "Epoch: 1, Iteration: 300, Loss: 0.8551573157310486\n",
            "Epoch: 1, Iteration: 310, Loss: 1.0717575550079346\n",
            "Epoch: 1, Iteration: 320, Loss: 1.0713647603988647\n",
            "Epoch: 1, Iteration: 330, Loss: 1.1262638568878174\n",
            "Epoch: 1, Iteration: 340, Loss: 1.2575337886810303\n",
            "Epoch: 1, Iteration: 350, Loss: 1.0937325954437256\n",
            "Epoch: 1, Iteration: 360, Loss: 1.1133478879928589\n",
            "Epoch: 1, Iteration: 370, Loss: 1.1495579481124878\n",
            "Epoch: 1, Iteration: 380, Loss: 1.098974585533142\n",
            "Epoch: 1, Iteration: 390, Loss: 1.0128328800201416\n",
            "Epoch: 1, Iteration: 400, Loss: 0.9937700033187866\n",
            "Epoch: 1, Iteration: 410, Loss: 1.0446081161499023\n",
            "Epoch: 1, Iteration: 420, Loss: 1.0633316040039062\n",
            "Epoch: 1, Iteration: 430, Loss: 1.1054191589355469\n",
            "Epoch: 1, Iteration: 440, Loss: 1.147698163986206\n",
            "Epoch: 1, Iteration: 450, Loss: 1.1910771131515503\n",
            "Epoch: 1, Iteration: 460, Loss: 0.9351442456245422\n",
            "Epoch: 1, Iteration: 470, Loss: 0.9931855201721191\n",
            "Epoch: 2, Iteration: 0, Loss: 1.032598614692688\n",
            "Epoch: 2, Iteration: 10, Loss: 1.0173368453979492\n",
            "Epoch: 2, Iteration: 20, Loss: 1.177734375\n",
            "Epoch: 2, Iteration: 30, Loss: 1.0133366584777832\n",
            "Epoch: 2, Iteration: 40, Loss: 1.0415418148040771\n",
            "Epoch: 2, Iteration: 50, Loss: 1.190178632736206\n",
            "Epoch: 2, Iteration: 60, Loss: 0.9822944402694702\n",
            "Epoch: 2, Iteration: 70, Loss: 1.002386450767517\n",
            "Epoch: 2, Iteration: 80, Loss: 1.0478659868240356\n",
            "Epoch: 2, Iteration: 90, Loss: 1.2289196252822876\n",
            "Epoch: 2, Iteration: 100, Loss: 0.9422727227210999\n",
            "Epoch: 2, Iteration: 110, Loss: 1.0082345008850098\n",
            "Epoch: 2, Iteration: 120, Loss: 1.0565135478973389\n",
            "Epoch: 2, Iteration: 130, Loss: 1.0657678842544556\n",
            "Epoch: 2, Iteration: 140, Loss: 1.1490668058395386\n",
            "Epoch: 2, Iteration: 150, Loss: 1.0301318168640137\n",
            "Epoch: 2, Iteration: 160, Loss: 1.0116000175476074\n",
            "Epoch: 2, Iteration: 170, Loss: 1.006996750831604\n",
            "Epoch: 2, Iteration: 180, Loss: 1.1967636346817017\n",
            "Epoch: 2, Iteration: 190, Loss: 1.0175360441207886\n",
            "Epoch: 2, Iteration: 200, Loss: 1.11543869972229\n",
            "Epoch: 2, Iteration: 210, Loss: 0.8584281206130981\n",
            "Epoch: 2, Iteration: 220, Loss: 1.003814697265625\n",
            "Epoch: 2, Iteration: 230, Loss: 1.0117677450180054\n",
            "Epoch: 2, Iteration: 240, Loss: 1.1114550828933716\n",
            "Epoch: 2, Iteration: 250, Loss: 0.9894967675209045\n",
            "Epoch: 2, Iteration: 260, Loss: 0.9821411967277527\n",
            "Epoch: 2, Iteration: 270, Loss: 1.3024576902389526\n",
            "Epoch: 2, Iteration: 280, Loss: 1.4510164260864258\n",
            "Epoch: 2, Iteration: 290, Loss: 0.6278077960014343\n",
            "Epoch: 2, Iteration: 300, Loss: 0.7434514164924622\n",
            "Epoch: 2, Iteration: 310, Loss: 0.8909160494804382\n",
            "Epoch: 2, Iteration: 320, Loss: 1.0375746488571167\n",
            "Epoch: 2, Iteration: 330, Loss: 1.0518138408660889\n",
            "Epoch: 2, Iteration: 340, Loss: 1.0710487365722656\n",
            "Epoch: 2, Iteration: 350, Loss: 1.1216464042663574\n",
            "Epoch: 2, Iteration: 360, Loss: 0.9881481528282166\n",
            "Epoch: 2, Iteration: 370, Loss: 1.22343111038208\n",
            "Epoch: 2, Iteration: 380, Loss: 1.1028015613555908\n",
            "Epoch: 2, Iteration: 390, Loss: 0.858686089515686\n",
            "Epoch: 2, Iteration: 400, Loss: 0.8747425675392151\n",
            "Epoch: 2, Iteration: 410, Loss: 1.0895164012908936\n",
            "Epoch: 2, Iteration: 420, Loss: 0.9243095517158508\n",
            "Epoch: 2, Iteration: 430, Loss: 1.2680195569992065\n",
            "Epoch: 2, Iteration: 440, Loss: 1.1524405479431152\n",
            "Epoch: 2, Iteration: 450, Loss: 1.1627923250198364\n",
            "Epoch: 2, Iteration: 460, Loss: 0.7633665800094604\n",
            "Epoch: 2, Iteration: 470, Loss: 0.9144500494003296\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.88      0.64       218\n",
            "           1       0.51      0.31      0.39        89\n",
            "           2       0.58      0.13      0.21       167\n",
            "\n",
            "    accuracy                           0.51       474\n",
            "   macro avg       0.53      0.44      0.41       474\n",
            "weighted avg       0.53      0.51      0.44       474\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Baseline models\n",
        "#BERT uncased/base\n",
        "import torch\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Labeled Dataset/climate_agreement.csv')\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and padding\n",
        "max_length = 128\n",
        "df['tokens_parent'] = df['body_parent'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n",
        "df['tokens_child'] = df['body_child'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_length, truncation=True))\n",
        "df['tokens_parent'] = df['tokens_parent'].apply(lambda x: x + [0] * (max_length - len(x)))\n",
        "df['tokens_child'] = df['tokens_child'].apply(lambda x: x + [0] * (max_length - len(x)))\n",
        "\n",
        "# Train-Test Split\n",
        "train, test = train_test_split(df, test_size=0.2)\n",
        "train_x_parent = torch.tensor(train['tokens_parent'].tolist())\n",
        "train_x_child = torch.tensor(train['tokens_child'].tolist())\n",
        "train_y = torch.tensor(train['label'].tolist())\n",
        "test_x_parent = torch.tensor(test['tokens_parent'].tolist())\n",
        "test_x_child = torch.tensor(test['tokens_child'].tolist())\n",
        "test_y = torch.tensor(test['label'].tolist())\n",
        "\n",
        "\n",
        "train_data = TensorDataset(train_x_parent, train_x_child, train_y)\n",
        "train_loader = DataLoader(train_data, batch_size=4)\n",
        "test_data = TensorDataset(test_x_parent, test_x_child, test_y)\n",
        "test_loader = DataLoader(test_data, batch_size=4)\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Calculate Class Weights\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=[0, 1, 2], y=train['label'])\n",
        "class_weights = torch.tensor(class_weights).float().to(device)\n",
        "\n",
        "# Training Loop with Gradient Accumulation and Mixed Precision\n",
        "accumulation_steps = 4\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    for i, (x_parent, x_child, y) in enumerate(train_loader):\n",
        "        x_parent, x_child, y = x_parent.to(device), x_child.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs_parent = model(x_parent, labels=None)[0]\n",
        "            outputs_child = model(x_child, labels=None)[0]\n",
        "            outputs_combined = (outputs_parent + outputs_child) / 2\n",
        "\n",
        "            loss = CrossEntropyLoss(weight=class_weights)(outputs_combined, y)\n",
        "\n",
        "        # Gradient Accumulation\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i+1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f'Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x_parent, x_child, y in test_loader:\n",
        "        x_parent, x_child, y = x_parent.to(device), x_child.to(device), y.to(device)\n",
        "        outputs_parent = model(x_parent)[0]\n",
        "        outputs_child = model(x_child)[0]\n",
        "        outputs_combined = (outputs_parent + outputs_child) / 2\n",
        "\n",
        "        _, predicted = torch.max(outputs_combined.data, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(all_labels, all_preds))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}